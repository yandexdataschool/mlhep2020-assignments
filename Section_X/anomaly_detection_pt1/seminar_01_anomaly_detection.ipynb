{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import numpy.testing as np_testing\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sns.set(font_scale=1.5, rc={'figure.figsize':(11.7, 8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting functions, nothing to do here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_level_lines(model, data, labels, size=100):\n",
    "    def _expand(a, b, frac=.5, margin=1.):\n",
    "        return a - abs(a) * frac - margin, b + abs(b) * frac + margin\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    min_x, min_y = data.min(axis=0)\n",
    "    max_x, max_y = data.max(axis=0)\n",
    "    min_x, max_x = _expand(min_x, max_x)\n",
    "    min_y, max_y = _expand(min_y, max_y)\n",
    "\n",
    "    all_x = np.linspace(min_x, max_x, num=size)\n",
    "    all_y = np.linspace(min_y, max_y, num=size)\n",
    "    XX, YY = np.meshgrid(all_x, all_y)\n",
    "    test_data = np.c_[XX.ravel(), YY.ravel()]\n",
    "\n",
    "    try:\n",
    "        predictions = model.decision_function(test_data).reshape(size, size)\n",
    "        data_scores = model.predict(data)\n",
    "        anomaly_scores = model.decision_function(data)\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            predictions = model._decision_function(test_data).reshape(size, size)\n",
    "            data_scores = model._predict(data)\n",
    "            anomaly_scores = model._decision_function(data)\n",
    "        except AttributeError:\n",
    "            predictions = model.predict_proba(test_data)[:, 0].reshape(size, size)\n",
    "            data_scores = model.predict(data)\n",
    "            anomaly_scores = model.predict_proba(data)[:, 0]\n",
    "\n",
    "    plt.contourf(all_x, all_y, predictions, cmap=plt.cm.coolwarm)\n",
    "\n",
    "    threshold = anomaly_scores[data_scores==1.0].min()\n",
    "    plt.contour(XX, YY, predictions, levels=[threshold], linewidths=2, colors='darkred')\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels)\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([min_x,max_x])\n",
    "    axes.set_ylim([min_y,max_y])\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "\n",
    "def plot_results(y_test, probabilities):\n",
    "    # plot ROC and PR curves\n",
    "    fpr, tpr, _ = roc_curve(y_test, probabilities)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, probabilities)\n",
    "\n",
    "    fig, (ax_roc, ax_pr_rec) = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_size_inches(15, 5)\n",
    "\n",
    "    # roc\n",
    "    ax_roc.plot(fpr, tpr, linewidth=3)\n",
    "    ax_roc.set_xlabel('FPR')\n",
    "    ax_roc.set_ylabel('TPR')\n",
    "\n",
    "    ax_roc.grid(True)\n",
    "    ax_roc.xaxis.label.set_fontsize(20)\n",
    "    ax_roc.yaxis.label.set_fontsize(20)\n",
    "\n",
    "    ax_roc.fill_between(fpr, tpr, 0, alpha=0.1)\n",
    "\n",
    "    # precision-recall\n",
    "    ax_pr_rec.plot(recall, precision, linewidth=3)\n",
    "    ax_pr_rec.set_xlabel('Recall')\n",
    "    ax_pr_rec.set_ylabel('Precision')\n",
    "\n",
    "    ax_pr_rec.grid(True)\n",
    "    ax_pr_rec.xaxis.label.set_fontsize(20)\n",
    "    ax_pr_rec.yaxis.label.set_fontsize(20)\n",
    "\n",
    "    ax_pr_rec.fill_between(recall, precision, 0, alpha=0.1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Anomaly detection in the wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we want to detect anomalies?\n",
    "\n",
    "* data filtration;\n",
    "* unforeseen detector failure detection;\n",
    "* search for interesting physics interactions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Toy data generation\n",
    "\n",
    "Let's generate some moons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_moons(train_size_pos=64, train_size_neg=4, test_size=64):\n",
    "    from sklearn.datasets import make_moons\n",
    "\n",
    "    get_pos = lambda size, seed: make_moons(n_samples=size, noise=0.05, random_state=seed)[0].astype('float32')\n",
    "\n",
    "    data_pos = get_pos(train_size_pos, random_seed)\n",
    "    data_pos_test = get_pos(test_size, random_seed + 2)\n",
    "\n",
    "    center = np.array([0.5, 0.25], dtype='float32')\n",
    "    X_range = np.array([\n",
    "      [-1.25, 2.25],\n",
    "      [-0.75, 1.25],\n",
    "    ], dtype='float32')\n",
    "\n",
    "    np.random.seed(random_seed + 3)\n",
    "    \n",
    "    def get_neg(n):\n",
    "        length = np.sqrt(np.random.uniform(1., 4., size=n))\n",
    "        angle = np.pi * np.random.uniform(0, 2, size=n)\n",
    "        x = length * np.cos(angle)\n",
    "        y = length * np.sin(angle)\n",
    "        return np.vstack((x, y)).T + center\n",
    "    \n",
    "    data_neg = get_neg(train_size_neg)\n",
    "    data_neg_test = get_neg(test_size)\n",
    "\n",
    "    data_train = np.concatenate([\n",
    "      data_pos,\n",
    "      data_neg\n",
    "    ], axis=0)\n",
    "    \n",
    "    data_test = np.concatenate([\n",
    "      data_pos_test,\n",
    "      data_neg_test\n",
    "    ], axis=0)\n",
    "\n",
    "    labels_train = np.concatenate([\n",
    "      np.ones(data_pos.shape[0], dtype='float32'),\n",
    "      np.zeros(data_neg.shape[0], dtype='float32')\n",
    "    ])\n",
    "    labels_test = np.concatenate([\n",
    "      np.ones(data_pos_test.shape[0], dtype='float32'),\n",
    "      np.zeros(data_neg_test.shape[0], dtype='float32')\n",
    "    ])\n",
    "    return data_train, labels_train, data_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = create_dataset_moons(\n",
    "    train_size_pos=1024, train_size_neg=16, test_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X_train.T, c=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection problem statement: train function $\\hat{f}:\\mathcal{X}\\to \\mathbb{R}$ on dataset $\\mathcal{S}$, such that\n",
    "\n",
    "  * $\\hat{f}(x) \\leq 0$, where $x$ -- **normal** observation, $\\hat{f}(x) > 0$ -- **anomal**;\n",
    "\n",
    "  * with low rate of **misses**: $\\hat{f}(x) \\leq 0$ for **anomal** $x$;\n",
    "\n",
    "  * and low rate of **false alarms**: $\\hat{f}(x) > 0$ for **normal** $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact representation $\\mathcal{S}$ could vary depending on problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**...sometimes we have labels**\n",
    "\n",
    "  * i.e. $\\mathcal{S} = (x_i, y_i)_{i=1}^m$ where $y_i\\in \\{\\pm 1\\}$ are __anomaly__ labels;\n",
    "  * <span style=\"color: red\">**BUT** </span> normal observations are dominant class:\n",
    "  $$\n",
    "    \\overbrace{\\lvert i\\colon y_i = +1 \\rvert}^{n_+}\n",
    "        \\ll \\overbrace{\\lvert i\\colon y_i = -1 \\rvert}^{n_-}\n",
    "    \\,. $$\n",
    "\n",
    "\n",
    "$\\color{red}{\\Rightarrow}$ **imbalanced classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...sometimes we don't have labels\n",
    "\n",
    "  * $\\mathcal{S} = (x_i)_{i=1}^m$ -- **no labels!**;\n",
    "\n",
    "  * predict $\\alpha \\in (0, 1)$ -- level of anomalies.\n",
    "\n",
    "\n",
    "$\\color{red}{\\Rightarrow}$ **outlier detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use accuracy, precision, recall and $f1$-score to assess the quality of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pr_rc](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png)\n",
    "\n",
    "\n",
    "$$\n",
    "    F_\\beta\n",
    "        = (1 + \\beta^2)\n",
    "            \\frac{\\text{Precision} \\cdot \\text{Recall}}\n",
    "                 {\\beta^2 \\, \\text{Precision} + \\text{Recall}}\n",
    "        = \\frac{\\beta + \\beta^{-1}}{\\beta\\frac{1}{\\text{Recall}} + \\beta^{-1}\\frac{1}{\\text{Precision}}}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Elliptic Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = EllipticEnvelope(assume_centered=False, contamination=0.5)\n",
    "\n",
    "model.fit(X_train)\n",
    "\n",
    "plot_level_lines(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EllipticEnvelope(contamination=0.01)\n",
    "model.fit(X_train)\n",
    "predictions_elliptic = model.decision_function(X_test)\n",
    "labels_elliptic = model.predict(X_test)\n",
    "\n",
    "\n",
    "plot_results(y_test, predictions_elliptic)\n",
    "\n",
    "metrics_elliptic = classification_report(y_test, (predictions_elliptic > 0) * 1, output_dict=True)\n",
    "print(classification_report(y_test, (labels_elliptic > 0) * 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "* simple to use;\n",
    "* interpretable decision boundary. \n",
    "\n",
    "\n",
    "Cons:\n",
    "\n",
    "* good only for single mode distributions;\n",
    "* covariance matrix could become singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3 Isolation Forest\n",
    "\n",
    "At the basis of the Isolation Forest algorithm there is the tendency of anomalous instances in a dataset to be easier to separate from the rest of the sample (isolate), compared to normal points. () Wiki\n",
    "\n",
    "![isoforest](https://i.stack.imgur.com/O59d4.png)\n",
    "\n",
    "![](https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2019/08/IsolationForest1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = IsolationForest(n_estimators=100,\n",
    "                        contamination=0.1,\n",
    "                        max_features=1.0,\n",
    "                        max_samples=1.0,\n",
    "                        bootstrap=True,\n",
    "                        random_state=0)\n",
    "model.fit(X_train)\n",
    "\n",
    "plot_level_lines(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsolationForest()\n",
    "model.fit(X_train)\n",
    "predictions_isolation = model.decision_function(X_test)\n",
    "labels_isolation = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, predictions_isolation)\n",
    "\n",
    "metrics_isolation = classification_report(y_test, (labels_isolation > 0) * 1, output_dict=True)\n",
    "print(classification_report(y_test, (labels_isolation > 0) * 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "* Robust.\n",
    "\n",
    "Cons:\n",
    "* hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.4 Local Outlier Factor\n",
    "\n",
    "The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. (c) wiki\n",
    "\n",
    "* introduce local density variable, that inversely proportional to the mean distance to k nearest neighbours;\n",
    "\n",
    "* compair local density of the object with local density for neighbours;\n",
    "\n",
    "* clalculate local anomaly score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4e/LOF-idea.svg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = LocalOutlierFactor(n_neighbors=20, \n",
    "                           contamination=0.1,\n",
    "                           metric='minkowski',\n",
    "                           novelty=True,\n",
    "                           p=2)\n",
    "model.fit(X_train)\n",
    "\n",
    "plot_level_lines(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocalOutlierFactor(contamination=0.02)\n",
    "model.fit(X_train)\n",
    "predictions_lof = model._decision_function(X_test)\n",
    "labels_lof = predictions_lof\n",
    "\n",
    "\n",
    "plot_results(y_test, predictions_lof)\n",
    "\n",
    "metrics_lof = classification_report(y_test, (labels_lof > 0) * 1, output_dict=True)\n",
    "print(classification_report(y_test, (labels_lof > 0) * 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "    \n",
    "* non-parametric method.\n",
    "\n",
    "Cons:\n",
    "    \n",
    "* affected by the curse of dimensionality;\n",
    "* can't distinguish anomaly clusters from normal clusters; \n",
    "* need to be trained for each new datasample, i.e. long inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.5 One Class SVM\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "  & \\underset{\\rho, f\\in \\mathcal{H}}{\\min}\n",
    "    & & \\tfrac12 \\|f\\|^2 - \\rho\n",
    "        + \\tfrac1{m \\nu} \\sum_{i=1}^m \\max\\bigl\\{\n",
    "            0, \\rho - f(x_i) \\bigr\\}\\,,\n",
    "\\end{aligned}\n",
    "\n",
    "Visualization for linear One-Class SVM: http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/\n",
    "\n",
    "Visualization of RBF One-Class SVM: https://bitquill.net/blog/quick-hack-visualizing-rbf-bandwidth/\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Andre_Marquand/publication/51462279/figure/fig2/AS:269979568308226@1441379553695/Illustration-of-the-OC-SVM-with-RBF-kernel-In-this-case-finding-the-smallest-hypersphere.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = OneClassSVM(nu=0.5, kernel='rbf', gamma=10.)\n",
    "\n",
    "model.fit(X_train)\n",
    "\n",
    "plot_level_lines(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassSVM(nu=0.5, kernel='rbf', gamma=10.)\n",
    "model.fit(X_train)\n",
    "predictions_svm = model.decision_function(X_test)\n",
    "labels_svm = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, predictions_svm)\n",
    "metrics_svm = classification_report(y_test, (labels_svm > 0) * 1, output_dict=True)\n",
    "print(classification_report(y_test, (labels_svm > 0) * 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "* non-parametric method;\n",
    "* kernel can be defined for any type of object\n",
    "    * graphs, texts;\n",
    "* can be extremely powerful if kernel appropriately chosen.\n",
    "\n",
    "Cons:\n",
    "* computationally demand during training;\n",
    "* stores training dataset in-memory for inference stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "estimator = XGBClassifier()\n",
    "\n",
    "grid = {\n",
    "    \"learning_rate\" : np.logspace(-2., 0., 10),\n",
    "    \"n_estimators\": np.linspace(10, 200, 5, dtype=np.int)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use statified $k$-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "st_kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv_grid = GridSearchCV(estimator, grid, scoring=f1_scorer, cv=st_kfold, n_jobs=-1, verbose=2)\n",
    "\n",
    "cv_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_classifier = cv_grid.best_estimator_\n",
    "xgboost_test_pred = xgboost_classifier.predict_proba(X_test)[:, 1]\n",
    "plot_level_lines(xgboost_classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, xgboost_test_pred)\n",
    "metrics_xgboost = classification_report(y_test, (xgboost_test_pred > 1.) * 1., output_dict=True)\n",
    "print(classification_report(y_test, (xgboost_test_pred > 0.5) * 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Profile quality of methods as a function of threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest_reports = {}\n",
    "xgb_reports = {}\n",
    "train_size_negs = np.logspace(0.5, 3, 20, dtype=np.int)\n",
    "\n",
    "for train_size_neg in tqdm.tqdm_notebook(train_size_negs):\n",
    "    X_train, y_train, _, _ = create_dataset_moons(\n",
    "        train_size_pos=1024, train_size_neg=train_size_neg, test_size=512\n",
    "    )\n",
    "    xgb = XGBClassifier(learning_rate=0.07, n_estimators=57)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_test = xgb.predict(X_test)\n",
    "    xgb_reports[train_size_neg] = classification_report(y_test, xgb_test, output_dict=True)\n",
    "    \n",
    "    isolation_forest = IsolationForest()\n",
    "    isolation_forest.fit(X_train)\n",
    "    isolation_forest_reports[train_size_neg] = classification_report(y_test, (isolation_forest.predict(X_test) > 0) * 1, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, sharex=True, figsize=(20, 22))\n",
    "i = 0\n",
    "\n",
    "for average in [\"macro avg\", \"weighted avg\"]:\n",
    "    for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "        xgb_metric = []\n",
    "        isolated_metric = []\n",
    "        for train_size_neg in train_size_negs:\n",
    "            xgb_metric.append(\n",
    "                xgb_reports[train_size_neg][average][metric]\n",
    "            )\n",
    "            isolated_metric.append(\n",
    "                isolation_forest_reports[train_size_neg][average][metric]\n",
    "            )\n",
    "        ax[i % 3][i // 3].plot((train_size_negs) / (1024 + train_size_negs), xgb_metric, label=\"XGBoost\")\n",
    "        ax[i % 3][i // 3].plot((train_size_negs) / (1024 + train_size_negs), isolated_metric, label=\"Isolated Forest\")\n",
    "        ax[i % 3][i // 3].set_title(\"{} {}\".format(average, metric))\n",
    "        ax[i % 3][i // 3].legend()\n",
    "        ax[i % 3][i // 3].set_xlabel(\"Rate of anomalies in train set\")\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
