{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Learning to pivot, part 2\n",
    "## Unconditional vs conditional pivoting\n",
    "\n",
    "Main paper: https://arxiv.org/abs/1611.01046\n",
    "\n",
    "This example demonstrates difference between conditional and unconditional pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlhep2019\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess as sp\n",
    "    result = sp.run(\n",
    "        ['pip3', 'install', 'git+https://github.com/yandexdataschool/mlhep2019.git'],\n",
    "        stdout=sp.PIPE, stderr=sp.PIPE\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(result.stdout.decode('utf-8'))\n",
    "        print(result.stderr.decode('utf-8'))\n",
    "    \n",
    "    import mlhep2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from mlhep2019.pivot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_gpu():\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount\n",
    "    nvmlInit()\n",
    "\n",
    "    return np.argmax([\n",
    "        nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i)).free\n",
    "        for i in range(nvmlDeviceGetCount())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_id = get_free_gpu()\n",
    "    device = 'cuda:%d' % (get_free_gpu(), )\n",
    "    print('Selected %s' % (device, ))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('WARNING: using cpu!')\n",
    "\n",
    "### please, don't remove the following line\n",
    "x = torch.tensor([1], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUISANCE_SIZE = 5\n",
    "def get_data(size = 1024):\n",
    "    nuisance_int = np.random.randint(0, NUISANCE_SIZE, size=(size, ))\n",
    "    nuisance = nuisance_int.astype('float32') / 4\n",
    "    \n",
    "    xs = np.random.uniform(-1, 1, size=(size, ))\n",
    "    ys = nuisance + np.random.normal(size=(size, )) * 0.05\n",
    "    \n",
    "    data = np.stack([xs, ys]).astype('float32').T\n",
    "    \n",
    "    labels = np.where((xs + 1) / 2 < ys, 1, 0).astype('float32')\n",
    "\n",
    "    return data, labels, nuisance_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, labels_train, nuisance_train = get_data(size=1024)\n",
    "data_test, labels_test, nuisance_test = get_data(size=128 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_train[labels_train < 0.5, 0], data_train[labels_train < 0.5, 1], label='class 0')\n",
    "plt.scatter(data_train[labels_train > 0.5, 0], data_train[labels_train > 0.5, 1], label='class 1')\n",
    "plt.xlabel('$x_1$', fontsize=14)\n",
    "plt.ylabel('$x_2$', fontsize=14)\n",
    "plt.title('Toy data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, grid = make_grid(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, z_train = [\n",
    "    torch.from_numpy(tensor).to(device)\n",
    "    for tensor in (data_train, labels_train, nuisance_train)\n",
    "]\n",
    "\n",
    "X_test, y_test, z_test = [\n",
    "    torch.from_numpy(tensor).to(device)\n",
    "    for tensor in (data_test, labels_test, nuisance_test)\n",
    "]\n",
    "\n",
    "G = torch.from_numpy(grid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = torch.utils.data.TensorDataset(X_test, y_test, z_test)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1024, shuffle=False)\n",
    "\n",
    "dataset_grid = torch.utils.data.TensorDataset(G)\n",
    "dataloader_grid = torch.utils.data.DataLoader(dataset_grid, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, loader):\n",
    "    with torch.no_grad():\n",
    "        return np.concatenate([\n",
    "            torch.sigmoid(model(batch[0])).to('cpu').detach().numpy()\n",
    "            for batch in loader\n",
    "        ], axis=0)\n",
    "\n",
    "test_predictions = lambda model: get_predictions(model, dataloader_test)\n",
    "grid_predictions = lambda model: get_predictions(model, dataloader_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Unmodified classification\n",
    "\n",
    "Here we define a simple classifier:\n",
    "```\n",
    "Input(2 units) -> DenseLayer(64 units) -> DenseLayer(32 units) -> DenseLayer(1 unit)\n",
    "```\n",
    "\n",
    "**Note:** we don't use any activation function for the output layer, however, at the same time with use `BCEWithLogitsLoss` loss as it is more computationally stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, activation=torch.nn.Softplus()):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(2, 64)\n",
    "        self.layer2 = torch.nn.Linear(64, 32)\n",
    "        self.head = torch.nn.Linear(32, 1)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        result = X\n",
    "        result = self.activation(self.layer1(result))\n",
    "        result = self.activation(self.layer2(result))\n",
    "\n",
    "        return torch.flatten(\n",
    "            self.head(result)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_classification = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 128\n",
    "num_batches = data_train.shape[0] // 32\n",
    "\n",
    "losses = np.zeros(shape=(num_epoches, num_batches))\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for i in tqdm(range(num_epoches)):\n",
    "    for j in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "        X_batch, y_batch = X_train[indx], y_train[indx]\n",
    "        \n",
    "        predictions = classifier(X_batch)\n",
    "\n",
    "        loss = loss_fn_classification(predictions, y_batch)\n",
    "        losses[i, j] = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(classifier=losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let's pivot\n",
    "\n",
    "In order to make predictions of the classifier independent from nuisance parameters, an adversary is introduced.\n",
    "The idea is similar to the main principle of GAN - seek for the solution that maximizes minimum of the adversary loss.\n",
    "\n",
    "If classifier utilises information about nuisance parameters to make predictions, then its predictions are dependent on nuisance parameters. This information is most probably coming from dependencies between nuisance parameters and the training features, therefore, just excluding nuisance parameters from the training features is typically not enough.\n",
    "\n",
    "Adversary is trained to predict nuisance parameters given output of the classifier. A dependency between nuisance parameters and predictions means that adversary is able to learn it (i.e. achieve minimum of the loss lower than loss of the constant). Maxumum of the minimum of the adversary loss is achieved only when there is not any dependencies between predictions and nusiances.\n",
    "\n",
    "\n",
    "More formally, adversary loss is given by:\n",
    "$$\\mathcal{L}_{\\mathrm{adv}}(\\theta, \\psi) = -\\mathbb{E}_{x, z} \\log P_\\psi(z \\mid f_\\theta(x)) \\to_\\psi \\min;$$\n",
    "while the classifier is trained to minimize the following loss:\n",
    "$$\\mathcal{L}_{\\mathrm{clf}} = \\left[-\\mathbb{E}_{x, y} \\log P_\\theta(y \\mid x)\\right] - \\left[ \\min_\\psi \\mathcal{L}_\\mathrm{adv}(\\theta, \\psi)\\right] \\to_\\theta \\min;$$\n",
    "where:\n",
    "- $f_\\theta$ and $P_\\theta$ - classifier with parameters $\\theta$ and probability distribution that corresponds to it;\n",
    "- $P_\\psi$ - probability distribution that corresponds to the output of adversary;\n",
    "\n",
    "Note the minus sign before the second term in $\\mathcal{L}_{\\mathrm{clf}}$.\n",
    "\n",
    "The training procedure is similar to that of GAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(torch.nn.Module):\n",
    "    def __init__(self, activation=torch.nn.Softplus()):\n",
    "        super(Adversary, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(1, 128)\n",
    "        self.head = torch.nn.Linear(128, NUISANCE_SIZE)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        result = X\n",
    "        result = self.activation(self.layer1(result))\n",
    "\n",
    "        return torch.squeeze(\n",
    "            self.head(result),\n",
    "            dim=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_classifier = Classifier().to(device)\n",
    "adversary = Adversary().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_pivoted_classification = torch.nn.BCEWithLogitsLoss()\n",
    "loss_fn_adversary = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Warning:** be careful using optimizers with an internal state for adversarial optimization problems ($\\max \\min$ problems): almost all of the popular optimizers have an internal state  (except for SGD). After performing an optimization step for the generator, optimization problem for the adversary changes, thus, previously accumulated internal state might become invalid. This might lead to the noticable associlations in the learning curves. Alternatively, it might result in the generator (classifier in our case) and the adversary going in circles, which appears as if they have converged, which is especially difficult to detect; or collapse of the generator, as improper internal state of the discriminator optimizer slows its convergance. \n",
    "\n",
    "One might avoid these effects by setting learning rate of the adversary optimizer to a low enough value and/or train the adversary longer.\n",
    "\n",
    "One can use any optimizer for the generator (classifier in our case), provided that the adversary has enough time to converge.\n",
    "\n",
    "From practical experience, optimizers that use $l_\\infty$ (adamax, AMSGrad etc) norm perform well. Nevertheless, when in doubt, use SGD for the adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_pivoted_classifier = torch.optim.Adam(pivoted_classifier.parameters(), lr=1e-3)\n",
    "optimizer_adversary = torch.optim.Adamax(adversary.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 128\n",
    "num_batches = data_train.shape[0] // 32\n",
    "\n",
    "losses_clf = np.zeros(shape=(num_epoches, num_batches))\n",
    "losses_adv = np.zeros(shape=(num_epoches, num_batches))\n",
    "\n",
    "for i in tqdm(range(num_epoches)):\n",
    "    for j in range(num_batches):\n",
    "        ### training adversary\n",
    "        for k in range(4):\n",
    "            ### generating batch\n",
    "            indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "            X_batch, z_batch = X_train[indx], z_train[indx]\n",
    "            \n",
    "            optimizer_adversary.zero_grad()\n",
    "\n",
    "            predictions = pivoted_classifier(X_batch)\n",
    "            nuisance_predictions = adversary(torch.unsqueeze(predictions, dim=1))\n",
    "            loss_adversary = loss_fn_adversary(nuisance_predictions, z_batch)\n",
    "            \n",
    "            loss_adversary.backward()\n",
    "            optimizer_adversary.step()\n",
    "\n",
    "        optimizer_pivoted_classifier.zero_grad()\n",
    "        \n",
    "        ### generating batch\n",
    "        indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "        X_batch, y_batch, z_batch = X_train[indx], y_train[indx], z_train[indx]\n",
    "        \n",
    "        ### training classifier\n",
    "        predictions = pivoted_classifier(X_batch)\n",
    "        nuisance_predictions = adversary(torch.unsqueeze(predictions, dim=1))\n",
    "\n",
    "        loss_classifier = loss_fn_pivoted_classification(predictions, y_batch)\n",
    "        loss_adversary = loss_fn_adversary(nuisance_predictions, z_batch)\n",
    "\n",
    "        losses_clf[i, j] = loss_classifier.item()\n",
    "        losses_adv[i, j] = loss_adversary.item()\n",
    "\n",
    "        joint_loss = loss_classifier - 2 * loss_adversary\n",
    "\n",
    "        joint_loss.backward()\n",
    "        optimizer_pivoted_classifier.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(epoch=i, classifier=losses_clf, adversary=losses_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you look closely, you will see tiny (sometimes not tiny) associlations - note, how adamax stops them (at least, tries to). Try different optimizer (e.g. adam, adagrad) or decreasing number of adversary training steps for more pronounced effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Conditional pivoting\n",
    "\n",
    "Sometimes it is desirable to make predictions independent from the nuisance parameter within each class. Note, that this might still leave some dependency between nuisance and overall distribution of predictions.\n",
    "\n",
    "In this case we make adversary **conditional**, which in practice means simply adding target labels as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalAdversary(torch.nn.Module):\n",
    "    def __init__(self, activation=torch.nn.Softplus()):\n",
    "        super(ConditionalAdversary, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(2, 128)\n",
    "        self.head = torch.nn.Linear(128, NUISANCE_SIZE)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        result = X\n",
    "        result = self.activation(self.layer1(result))\n",
    "\n",
    "        return torch.squeeze(self.head(result), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_pivoted_classifier = Classifier().to(device)\n",
    "conditional_adversary = ConditionalAdversary().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_conditional_pivoted_classification = torch.nn.BCEWithLogitsLoss()\n",
    "loss_fn_conditional_adversary = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_conditional_pivoted_classifier = torch.optim.Adam(\n",
    "    conditional_pivoted_classifier.parameters(), lr=1e-3\n",
    ")\n",
    "optimizer_conditional_adversary = torch.optim.Adam(conditional_adversary.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 128\n",
    "num_batches = data_train.shape[0] // 32\n",
    "\n",
    "losses_clf = np.zeros(shape=(num_epoches, num_batches))\n",
    "losses_adv = np.zeros(shape=(num_epoches, num_batches))\n",
    "\n",
    "for i in tqdm(range(num_epoches)):\n",
    "    for j in range(num_batches):\n",
    "\n",
    "        ### training adversary\n",
    "        for k in range(4):\n",
    "            optimizer_conditional_adversary.zero_grad()\n",
    "            \n",
    "            indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "            X_batch, y_batch, z_batch = X_train[indx], y_train[indx], z_train[indx]\n",
    "\n",
    "            predictions = conditional_pivoted_classifier(X_batch)\n",
    "            \n",
    "            nuisance_predictions = conditional_adversary(\n",
    "                torch.stack([predictions, y_batch], dim=1)\n",
    "            )\n",
    "            loss_adversary = loss_fn_conditional_adversary(nuisance_predictions, z_batch)\n",
    "            \n",
    "            loss_adversary.backward()\n",
    "            optimizer_conditional_adversary.step()\n",
    "\n",
    "        optimizer_conditional_pivoted_classifier.zero_grad()\n",
    "        \n",
    "        indx = torch.randint(0, data_train.shape[0], size=(32, ))\n",
    "        X_batch, y_batch, z_batch = X_train[indx], y_train[indx], z_train[indx]\n",
    "        \n",
    "        ### training classifier\n",
    "        predictions = conditional_pivoted_classifier(X_batch)\n",
    "        nuisance_predictions = conditional_adversary(\n",
    "            torch.stack([predictions, y_batch], dim=1)\n",
    "        )\n",
    "\n",
    "        loss_classifier = loss_fn_conditional_pivoted_classification(predictions, y_batch)\n",
    "        loss_adversary = loss_fn_conditional_adversary(nuisance_predictions, z_batch)\n",
    "\n",
    "        losses_clf[i, j] = loss_classifier.item()\n",
    "        losses_adv[i, j] = loss_adversary.item()\n",
    "\n",
    "        joint_loss = loss_classifier - 2 * loss_adversary\n",
    "\n",
    "        joint_loss.backward()\n",
    "        optimizer_conditional_pivoted_classifier.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(classifier=losses_clf, adversary=losses_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "cross_entropy = lambda y, p: log_loss(y, p, eps=1e-6)\n",
    "accuracy = lambda y, p: np.mean(np.where(y > 0.5, 1, 0) == np.where(p > 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(nrows=1, ncols=3, figsize=(23, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('non-pivoted')\n",
    "\n",
    "draw_response(xs, ys, grid_predictions(classifier), data_train, labels_train)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('pivoted, unconditional')\n",
    "draw_response(xs, ys, grid_predictions(pivoted_classifier), data_train, labels_train)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('pivoted, conditional')\n",
    "draw_response(xs, ys, grid_predictions(conditional_pivoted_classifier), data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following figure shows dependency between predictions and the nuisance parameter:\n",
    "- each column correspond to a different model;\n",
    "- rows correspond to nuisance parameter bins;\n",
    "- each plot show distribution of model predictions within the corresponding nuisance bin.\n",
    "\n",
    "- $\\mathrm{MI}$ - (unconditional) mutual information between the nuisance parameter and model predictions.\n",
    "- $\\mathrm{MI}_i$ - mutual information between the nuisance parameter and model predictions **within** $i$-th class.\n",
    "\n",
    "**Note**, that the following Mutual Information estimates migh be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_prediction_hist([\n",
    "        test_predictions(classifier),\n",
    "        test_predictions(pivoted_classifier),\n",
    "        test_predictions(conditional_pivoted_classifier)\n",
    "    ],\n",
    "    nuisance_test,\n",
    "    labels=labels_test.astype('int'),\n",
    "    names=['non-pivoted', 'pivoted, unconditional', 'pivoted, conditional']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pivoted models tend to show worse (but flat) performance.\n",
    "If pivoted models shows an increased performance in some regions, then most likely the model is biased (i.e. low capacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_metric_plot([\n",
    "        test_predictions(classifier),\n",
    "        test_predictions(pivoted_classifier),\n",
    "        test_predictions(conditional_pivoted_classifier)\n",
    "    ],\n",
    "    labels_test, nuisance_test,\n",
    "    metric_fn=accuracy, metric_name='accuracy',\n",
    "    names=['non-pivoted', 'pivoted', 'conditional-pivoted'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_metric_plot([\n",
    "        test_predictions(classifier),\n",
    "        test_predictions(pivoted_classifier),\n",
    "        test_predictions(conditional_pivoted_classifier)\n",
    "    ],\n",
    "    labels_test, nuisance_test,\n",
    "    metric_fn=roc_auc_score, metric_name='ROC AUC',\n",
    "    names=['non-pivoted', 'pivoted', 'conditional-pivoted'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_metric_plot([\n",
    "        test_predictions(classifier),\n",
    "        test_predictions(pivoted_classifier),\n",
    "        test_predictions(conditional_pivoted_classifier)\n",
    "    ],\n",
    "    labels_test, nuisance_test,\n",
    "    metric_fn=cross_entropy, metric_name='cross-entropy', base_level=0.0,\n",
    "    names=['non-pivoted', 'pivoted', 'conditional-pivoted'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
