{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Something you'll actually use: Gradient Boosting machines\n",
    "From user point of view the competing toolkits are largely the same. Think BMW vs. Mercedes or Tensorflow 2 vs pytorch. In each there is clever engeneering, smart algorithm enchantments and ingenious heuristic tricks, but for common tasks the performance difference is different from paper to paper and is small for practical purposes.\n",
    "\n",
    "They all support **multithreading**, training on **GPU**,  have commited developer communities, **documentation**, and, subjectively, are great pieces of software. As of 2020 in chronological order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### XGBoost\n",
    "* Is the oldest and the most known\n",
    "* Multi-node Multi-GPU distributed training\n",
    "* Can [plot its trees](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting)\n",
    "\n",
    "#### Notable technical details [[paper](https://arxiv.org/pdf/1603.02754.pdf)]:\n",
    "* Can use [DART: Dropouts meet Multiple Additive Regression Trees](http://proceedings.mlr.press/v38/korlakaivinayak15.pdf)\n",
    "* Can use exact, approximate and histogram-based split search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Light GBM\n",
    "* (supposingly) Fast and memory-efficitent training\n",
    "* GPU training\n",
    "* [Distributed training](https://github.com/microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst)\n",
    "\n",
    "#### Notable [technical details](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst) [[paper](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)]\n",
    "* Gradient-based One-Side Sampling (GOSS)\n",
    "* Exclusive Feature Bundling (EFB). \n",
    "* LightGBM uses histogram-based algorithms, which bucket continuous feature (attribute) values into discrete bins\n",
    "* Leaf-wise (Best-first) Tree Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Catboost\n",
    "* Built-in handling of categorial features\n",
    "* Single-node Multi-GPU training\n",
    "* Is fast to run the prediction (we use in LHCb trigger)\n",
    "* [Fancy Jupyter training visualisation](https://catboost.ai/docs/features/visualization.html) (sadly, doesn't work in colab)\n",
    "\n",
    "#### Notable technical details [[paper](https://arxiv.org/pdf/1706.09516.pdf)]:\n",
    "* Uses oblivious trees. They are less flexible than the ordinary trees, but are a lot faster to evaluate\n",
    "\n",
    "![oblivious trees](https://github.com/yandexdataschool/mlhep2019/raw/master/notebooks/day-2/oblivious_tree.webp)\n",
    "\n",
    "* Ordered boosting: for calculating the residual on an example, uses a model trained without it\n",
    "* Uses histogram-based split selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_FEATURE_COLUMNS = ['ncl[0]', 'ncl[1]', 'ncl[2]', 'ncl[3]', 'avg_cs[0]',\n",
    "       'avg_cs[1]', 'avg_cs[2]', 'avg_cs[3]', 'ndof', 'MatchedHit_TYPE[0]',\n",
    "       'MatchedHit_TYPE[1]', 'MatchedHit_TYPE[2]', 'MatchedHit_TYPE[3]',\n",
    "       'MatchedHit_X[0]', 'MatchedHit_X[1]', 'MatchedHit_X[2]',\n",
    "       'MatchedHit_X[3]', 'MatchedHit_Y[0]', 'MatchedHit_Y[1]',\n",
    "       'MatchedHit_Y[2]', 'MatchedHit_Y[3]', 'MatchedHit_Z[0]',\n",
    "       'MatchedHit_Z[1]', 'MatchedHit_Z[2]', 'MatchedHit_Z[3]',\n",
    "       'MatchedHit_DX[0]', 'MatchedHit_DX[1]', 'MatchedHit_DX[2]',\n",
    "       'MatchedHit_DX[3]', 'MatchedHit_DY[0]', 'MatchedHit_DY[1]',\n",
    "       'MatchedHit_DY[2]', 'MatchedHit_DY[3]', 'MatchedHit_DZ[0]',\n",
    "       'MatchedHit_DZ[1]', 'MatchedHit_DZ[2]', 'MatchedHit_DZ[3]',\n",
    "       'MatchedHit_T[0]', 'MatchedHit_T[1]', 'MatchedHit_T[2]',\n",
    "       'MatchedHit_T[3]', 'MatchedHit_DT[0]', 'MatchedHit_DT[1]',\n",
    "       'MatchedHit_DT[2]', 'MatchedHit_DT[3]', 'Lextra_X[0]', 'Lextra_X[1]',\n",
    "       'Lextra_X[2]', 'Lextra_X[3]', 'Lextra_Y[0]', 'Lextra_Y[1]',\n",
    "       'Lextra_Y[2]', 'Lextra_Y[3]', 'NShared', 'Mextra_DX2[0]',\n",
    "       'Mextra_DX2[1]', 'Mextra_DX2[2]', 'Mextra_DX2[3]', 'Mextra_DY2[0]',\n",
    "       'Mextra_DY2[1]', 'Mextra_DY2[2]', 'Mextra_DY2[3]', 'FOI_hits_N', 'PT', 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../../share/data/1.6.2-boosting\"\n",
    "data = pd.read_csv(os.path.join(DATA_FOLDER, \"train_1_percent.csv.gz\"), index_col=\"id\",\n",
    "                   usecols=SIMPLE_FEATURE_COLUMNS+[\"id\", \"weight\", \"label\", \"kinWeight\"])\n",
    "\n",
    "# Just in case. It's available here:\n",
    "# https://www.kaggle.com/datasets/kazeev/idao2019muonid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT FOR COOPETITION\n",
    "# \n",
    "# weight = kinWeight*sWeight\n",
    "# since sWeights can be negative and not all ML software can handle negative weights,\n",
    "# we disregard the backgroud subtraction. When competing you might want to explore the ways to handle them\n",
    "#\n",
    "# We also use only 1% of the Coopetition training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
    "    data.loc[:, SIMPLE_FEATURE_COLUMNS].values,\n",
    "    data.label.values,\n",
    "    data.kinWeight,\n",
    "    test_size=0.4,\n",
    "    random_state=12421)\n",
    "\n",
    "X_test, X_val, y_test, y_val, w_test, w_val = train_test_split(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    w_val,\n",
    "    test_size=0.5,\n",
    "\n",
    "    random_state=5451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "import xgboost\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "GPU models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_gpu = {\n",
    "    \"catboost\": catboost.CatBoostClassifier(n_estimators=N_TREES, verbose=False, task_type='GPU'),\n",
    "    \"xgboost\": xgboost.XGBClassifier(n_estimators=N_TREES, tree_method=\"gpu_hist\"),\n",
    "    # LGB might be problematic\n",
    "    # \"lightgbm\": lightgbm.LGBMClassifier(n_estimators=N_TREES, device_type=\"gpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "CPU models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cpu = {\n",
    "    \"catboost\": catboost.CatBoostClassifier(n_estimators=N_TREES, verbose=False, thread_count=-1),\n",
    "    \"xgboost\": xgboost.XGBClassifier(n_estimators=N_TREES, tree_method=\"hist\", nthread=-1),\n",
    "    \"lightgbm\": lightgbm.LGBMClassifier(n_estimators=N_TREES, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def evaluate_models(models_dict):\n",
    "    for model_name, model in models_dict.items():\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train, sample_weight=w_train)\n",
    "        end = time.time()\n",
    "        print(\"{}; train time {:.3f} s; ROC AUC = {:.3f}\".format(\n",
    "              model_name,\n",
    "              end - start,\n",
    "              roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], sample_weight=w_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(models_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(models_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Please keep in mind that the comparison between CPU and GPU depends a lot of dataset size and the number of CPU cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A note on parameter tuning\n",
    "In a nutshell, parameter tuning is about bias-variance tradeoff. You tune strength of different regularizations - make model more flexible and more likely to overfit and visa versa. Cross-validation (or just holdout set validation) is your best friend here.\n",
    "\n",
    "The most important parameters usually are learning rate and the number of iterations. You want to specify the model complexity you can afford and then fully utilize said complexity.\n",
    "\n",
    "Next in importance, according to the authors of the libararies, are the regularizations, such as L2 and tree depth.\n",
    "\n",
    "We don't have time to thoroughly discuss all options of all packages - if you are interested in parameter tuning, please read the documentation of the library of choice: [xgboost](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html), [lightgbm](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html), [catboost](https://catboost.ai/docs/concepts/parameter-tuning.html)\n",
    "\n",
    "You don't have to tune the parameters manually! Just stay with us till the optimization on Saturday, 25th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1 (difficulty: semi-real life)\n",
    "The above train time comparison is not fair. We just ran training with default parameters - it is always possible that, if we take more iterations for some models and less for others, both timing and discrimination performances would arrange themselves differently. Your task is to do a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plot the ROC AUC as function of training time for different libraries, by varying just the number of iterations, leaving all other parameters to default values. Explore iterations count in `np.linspace(1, 50, num=5, dtype=np.int32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb0efb866ab72132d075a5bf0743a8ed",
     "grade": false,
     "grade_id": "e427a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "iterations_space = np.linspace(1, 50, num=5, dtype=np.int32)\n",
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "535c724e8f28724af303e98d14d0f7b8",
     "grade": false,
     "grade_id": "a745b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError\n",
    "# P.S. This comparison is still not good enough for a paper at a machine learning\n",
    "# conference. We didn't investigate the impact of other parameters, what if\n",
    "# by tuning regularization and, say, learning rate, we can change the winner?\n",
    "# The most proper way would be to do a full hyperparameter tuning - stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 2\n",
    "\n",
    "Write a function that selects the optimal learning rate. Early stopping is your good friend here: if a model has reached the maximum quality before hitting the iterations limit, the learning rate can be decreased, otherwise increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "eval_set = [(X_val, y_val)]\n",
    "model_xgb = xgboost.XGBClassifier(n_estimators=100500,\n",
    "                                  nthread=-1,\n",
    "                                  learning_rate=1.) # So that in this example it will quickly overfit\n",
    "model_xgb.fit(X_train, y_train,\n",
    "              sample_weight=w_train,\n",
    "              verbose=False,\n",
    "              eval_metric=\"auc\", # We use that metric for final scoring\n",
    "              early_stopping_rounds=10, # Try training this more rounds after reaching the best score\n",
    "              eval_set=eval_set,\n",
    "              sample_weight_eval_set=[w_val]\n",
    "             ) # Note that we use separate datasets for test and early stopping\n",
    "print(\"The trained model has only {} trees\".format(len(model_xgb.get_booster().get_dump())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "model_lgb = lightgbm.LGBMClassifier(n_estimators=100500,\n",
    "                                    n_jobs=-1,\n",
    "                                    learning_rate=1.)\n",
    "model_lgb.fit(X_train, y_train,\n",
    "              verbose=False,\n",
    "              eval_metric=\"auc\",\n",
    "              early_stopping_rounds=10,\n",
    "              eval_set=eval_set,\n",
    "              eval_sample_weight=[w_val])\n",
    "\n",
    "print(\"The trained model has only {} trees\".format(\n",
    "      model_lgb.booster_.num_trees()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "model_cb = catboost.CatBoostClassifier(n_estimators=100500,\n",
    "                                       verbose=False, task_type='GPU',\n",
    "                                       learning_rate=1.,\n",
    "                                       # Note how unlike the other packages\n",
    "                                       # eval_metric is passed to constructor\n",
    "                                       # instead of fit method\n",
    "                                       eval_metric=\"AUC\")\n",
    "\n",
    "# not the way we pass the validaton weights to catboost\n",
    "val_cb = catboost.Pool(data=X_val, label=y_val, weight=w_val)\n",
    "model_cb.fit(X_train, y_train,\n",
    "              verbose=False,\n",
    "              early_stopping_rounds=10,\n",
    "              eval_set=val_cb)\n",
    "\n",
    "print(\"The trained model has only {} trees\".format(\n",
    "      model_cb.tree_count_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When a function gets a model in parameters, it will be reference\n",
    "# it would be bad style to change it, so here is how to make a copy\n",
    "from sklearn.base import clone\n",
    "model_copy = clone(model_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f395087b22d47f3a243d491c9d992699",
     "grade": false,
     "grade_id": "79c0db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_optimal_learning_rate(model, verbose:bool=False, return_best_model:bool=False, tolerance:float=0.01):\n",
    "    \"\"\"\n",
    "    Uses early stopping and bisection method to efficiently find the optimal learning rate.\n",
    "    The funciton finds the highest learning rate for which early stopping doesnt' kick in.\n",
    "    Arg:\n",
    "        model: a model with sklearn-type interface.\n",
    "           Supports XGBClassifier, LGBMClassifier, CatBoostClassifier\n",
    "        verbose: print the progress\n",
    "        return_best_model: if set, return (learning_rate, best_model)\n",
    "           if not set, return just learning_rate\n",
    "        tolerance: the maximum difference betweent the truly optimal and retured value\n",
    "    Returns:\n",
    "        optimal learning rate, if return_best_model is false\n",
    "        (optimal learning rate, best model) if return_best_model true\n",
    "    \"\"\"\n",
    "    # Use eval_set, w_val and val_cb for measuring the performance\n",
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_xgb, clf_xgb = find_optimal_learning_rate(\n",
    "    xgboost.XGBClassifier(n_estimators=N_TREES, nthread=-1), verbose=True, return_best_model=True)\n",
    "print(lr_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e006fd089619160e548f942f0f3f0381",
     "grade": true,
     "grade_id": "1f7626",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the test bounds are loose to accomodate for flucatuations\n",
    "assert 0.01 < lr_xgb < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lgb, clf_lgb = find_optimal_learning_rate(\n",
    "    lightgbm.LGBMClassifier(n_estimators=N_TREES, n_jobs=-1), verbose=True, return_best_model=True)\n",
    "print(lr_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "333f9cf9459f01b2dcce94f0c8e04e76",
     "grade": true,
     "grade_id": "ae93d9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the test bounds are loose to accomodate for flucatuations\n",
    "assert 0 < lr_lgb < 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cb, clf_cb = find_optimal_learning_rate(\n",
    "  catboost.CatBoostClassifier(n_estimators=N_TREES, verbose=False, task_type='GPU'),\n",
    "  verbose=True, return_best_model=True)\n",
    "print(lr_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7de2ad1815bb3d0a6b88cfee4d9afe73",
     "grade": true,
     "grade_id": "44e871",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the test bounds are loose to accomodate for flucatuations\n",
    "assert 0 < lr_cb < 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models({\n",
    "        \"xgb\": clf_xgb,\n",
    "        \"lgb\": clf_lgb,\n",
    "        \"cb\": clf_cb\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 3 (Long and optional)\n",
    "Compare the GPU and CPU performances on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ensembling - recap\n",
    "* Combining several models in a semi-clever way usually results in better quality\n",
    "* The most common ways are stacking, bagging and boosting\n",
    "* Stacking is how deep learning works - by using the output of one model  as features to another.\n",
    "* Stacking is a common thing in competititive data science, where it allows to get that 0.0001%. A common quick and dirty technique of averaging predictions of different models can be viewed as stacking with a linear model.\n",
    "* Boosted decision trees have quality competitive with deep learning on tabular data. In general (please don't quote me on this), there is a trend that the smaller the datasets, the worse will a fully-connected MLP perform when compared to gradient boosting. //Tabular here means without assumptions of structure in featueres, such as when they pixels in an image. E. g. [Higgs dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS) is tabular, but ImageNet is not. You can treat any dataset as tabular, but almost certainly ignoring the structure will lead to worse performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
