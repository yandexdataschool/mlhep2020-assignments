{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ensembles warm up: Random Forest\n",
    "Random forest fits multiple trees on random subsets of examples and features. They are better than raw decision trees almost in every respect - with the exceptions of interpretability and computational cost.\n",
    "\n",
    "Compared to boosting, the advantages are:\n",
    "* Harder to overfit. Overfitting will not be made worse by adding more trees.\n",
    "* Very easy to implement parallel training. If you ever have to do it by yourself for whatever reason...\n",
    "\n",
    "Compared to boosting, the disadvantage is worse quality.\n",
    "\n",
    "Please take your time to enjoy the beatiful smooth decision surface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "def plot_decision_surface(clf,\n",
    "                          X: np.ndarray,\n",
    "                          y: np.ndarray,\n",
    "                          grid_step: float=0.02,\n",
    "                          cmap='bwr',\n",
    "                          alpha:float=0.6,\n",
    "                          axes=None\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plot the decision surface of a classifier, visualize selected points\n",
    "    Args:\n",
    "      clf: a fitted model, must support predict method\n",
    "      X[n_examples, n_features]: points where to evaluate the classifier\n",
    "      y[n_examples]: true labels\n",
    "      grid_step: decision surface plottting grid\n",
    "      alpha: opacity of the decision surface\n",
    "      axes(matplotlib.axes._subplots.AxesSubplot): axes where plot, if None, a new figure is created\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the grid\n",
    "    x_top_left = X.min(axis=0) - 1\n",
    "    x_bottom_right = X.max(axis=0) + 1\n",
    "    grid_x0, grid_x1 = np.meshgrid(\n",
    "         np.arange(x_top_left[0], x_bottom_right[0], grid_step),\n",
    "         np.arange(x_top_left[1], x_bottom_right[1], grid_step)\n",
    "      )\n",
    "\n",
    "    # Calculate predictions on the grid\n",
    "    y_pred_grid = clf.predict(\n",
    "                        np.stack(\n",
    "                              [\n",
    "                                grid_x0.ravel(),\n",
    "                                grid_x1.ravel()\n",
    "                              ],\n",
    "                              axis=1\n",
    "                            )\n",
    "                      ).reshape(grid_x1.shape)\n",
    "\n",
    "    # Find optimal contour levels and make a filled\n",
    "    # contour plot of predictions\n",
    "    labels = np.sort(np.unique(y))\n",
    "    labels = np.concatenate([[labels[0] - 1],\n",
    "                             labels,\n",
    "                             [labels[-1] + 1]])\n",
    "    medians = (labels[1:] + labels[:-1]) / 2\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots()\n",
    "    axes.contourf(grid_x0, grid_x1, y_pred_grid, cmap=cmap, alpha=alpha,\n",
    "                 levels=medians)\n",
    "\n",
    "    # Scatter data points on top of the plot,\n",
    "    # with different styles for correct and wrong\n",
    "    # predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    axes.scatter(*X[y_pred==y].T, c=y[y_pred==y],\n",
    "                marker='o', cmap=cmap, s=10, label='correct')\n",
    "    axes.scatter(*X[y_pred!=y].T, c=y[y_pred!=y],\n",
    "                marker='x', cmap=cmap, s=50, label='errors')\n",
    "\n",
    "    # Dummy plot call to print the accuracy in the legend.\n",
    "    axes.plot([], [], ' ',\n",
    "             label='Accuracy = {:.3f}'.format(accuracy_score(y, y_pred)))\n",
    "    axes.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data should be in CoCalc. If it isn't, it can be downloaded here:\n",
    "#! wget https://github.com/yandexdataschool/mlhep2018/raw/master/day1-Mon/data.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you used wget\n",
    "# DATA_FOLDER = \".\"\n",
    "DATA_FOLDER = \"../../../share/data/1.6.1-ensembles\"\n",
    "muticlass_toy_data = np.load(os.path.join(DATA_FOLDER, \"data.npz\"))\n",
    "X_toy_multiclass_train, X_toy_multiclass_test, \\\n",
    "  y_toy_multiclass_train, y_toy_multiclass_test = \\\n",
    "    train_test_split(muticlass_toy_data[\"X\"], muticlass_toy_data[\"y\"],\n",
    "                     test_size=0.5, random_state=421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Note the n_jobs parameter: it defines the number of parallel training processes\n",
    "# -1 means to use all available\n",
    "random_forest = RandomForestClassifier(n_estimators=100,\n",
    "                                       min_samples_leaf=5,\n",
    "                                       n_jobs=-1, random_state=3421)\n",
    "\n",
    "random_forest.fit(X_toy_multiclass_train, y_toy_multiclass_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_surface(random_forest,\n",
    "                      X_toy_multiclass_test,\n",
    "                      y_toy_multiclass_test,\n",
    "                      cmap='rainbow', grid_step=0.4, axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# An interlude on uncertainty\n",
    "*  Quality of an ML algorithm \"in general\" is hard to define and, in complete absense of assumptions on the data, all algorithms have the same performance - [no free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)\n",
    "*  Performance of a **particular** trained model on a **particular** test dataset is, however, a simple statistical problem you all are familiar with. Example loss is an i. i. d. random variable, algorithm performance is its expeted value, this is it.\n",
    "* As AUC is not an expectation of some **per-example** loss, the statistics is a lot trickier. See the [paper](https://ieeexplore.ieee.org/document/6851192/) and a [python implementation](https://github.com/yandexdataschool/roc_comparison/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3 (difficulty: statistics 101)\n",
    "Compute the t-statistic and p-value and for hypothesis that random forest average accuracy is equal to decision tree accuracy on the `toy_multiclass_test` dataset\n",
    "* for the whole test dataset\n",
    "* for the subset consisting of the first 5 examples of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_forest = random_forest.predict(X_toy_multiclass_test)\n",
    "predictions_tree = DecisionTreeClassifier(random_state=234).fit(\n",
    "    X_toy_multiclass_train, y_toy_multiclass_train).predict(X_toy_multiclass_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1950b82890e563088bb228a5ead2e930",
     "grade": false,
     "grade_id": "cell-0d71c9311aa54f3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# For the benefit of the checker, please store the results in the following variables:\n",
    "# full_test_p_value and test_5_p_value\n",
    "# Your good friends here are\n",
    "# - the definition of accuracy mean(predicted_label == real_label)\n",
    "# - T-test and its Python implementation: scipy.stats.ttest_ind\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_test_p_value, test_5_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e9d48e7cf0be066d18800dbe5f24c23",
     "grade": true,
     "grade_id": "p_value",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The actual p-values you'll see are likely to be lower and higher respectively.\n",
    "# The assertion boundaries are very permissinve to account for possible fluctuations.\n",
    "assert(full_test_p_value < 1e-12)\n",
    "assert(test_5_p_value > 1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
