{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import norm\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pyro\n",
    "import numpy.testing as np_testing\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.special import expit, logit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sns.set(font_scale=1.5, rc={'figure.figsize':(11.7, 8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Seminar plan\n",
    "    \n",
    "    1. Fit binomial distribution to the space worms teeth distribution using Wasserstein divergence;\n",
    "    2. Fit 2D-normal distribution with Wasserstein;\n",
    "        1. Learn about Kantorovich-Rubinstein duality;\n",
    "            - Yet another minimax problem!\n",
    "        2. Learn about weights clipping as a way to ensure 1-Lipschitz;\n",
    "        3. Leatn about gradient penalty as a way to ensure 1-Lipschitz(extra, no grading)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Discrete example\n",
    "\n",
    "In this notebook we're going to take a look at a way of comparing two probability distributions called Kullback-Leibler Divergence. \n",
    "\n",
    "## 1.1 Space worms teeth distribution\n",
    "\n",
    "Suppose that we're space-scientists visiting a distant, new planet and we've discovered a species of biting worms that we'd like to study. We've found that these worms have 10 teeth, but because of all the chomping away, many of them end up missing teeth. After collecting many samples we have come to this empirical probability distribution of the number of teeth in each worm:\n",
    " \n",
    "![Space worms](https://images.squarespace-cdn.com/content/v1/54e50c15e4b058fc6806d068/1494401025139-ODE7CP2043TS1CO9MQSN/ke17ZwdGBToddI8pDm48kLuT3KTpMRZ2imBrzIWD9_5Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVG-_BClLJADi5Tjms1vR9XfE3ardhQXleMJTem2-1ZqRideLm3HbGNLisCtv4-dzhc/biting-worms.jpg?format=1000w)\n",
    "\n",
    "Picture and idea credits: https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the probability adds up to 1\n",
    "true_data = torch.tensor([0.1, 0.2, 0.11, 0.11, 0.05, 0.02, 0.03, 0.05, 0.11, 0.15, 0.07])\n",
    "n = true_data.shape[0]\n",
    "index = torch.arange(n).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(true_data.sum().item(), 1., err_msg='Your probability do not sum up to 1!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(index, true_data)\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth()')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hypothesis about the data\n",
    "\n",
    "###### Hypothesis 1: uniform distribution of space worms teeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_data = torch.full((n, ), 1.0 / n)\n",
    "\n",
    "plt.figure()\n",
    "# we can plot our approximated distribution against the original distribution\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, uniform_data, width=width, label='Uniform')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(index)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hypothesis 2: binomial distribution with unknown parameter $p$\n",
    "\n",
    "\n",
    "###### Introducing a new framework: Pyro\n",
    "Pyro is a flexible, scalable deep probabilistic programming library built on PyTorch. (c) https://github.com/pyro-ppl/pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro import distributions as distrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we estimate the parameter of the binomial distribution\n",
    "p = true_data.dot(index) / n\n",
    "print('p for binomial distribution:', p)\n",
    "binomial_dist = distrs.Binomial(total_count=n, probs=p)\n",
    "binom_data = binomial_dist.log_prob(index).exp()\n",
    "binom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(np.arange(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(index - width, true_data, width=width, label='True')\n",
    "plt.bar(index, uniform_data, width=width, label='Uniform')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth Number')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(index)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And here the math starts\n",
    "\n",
    "![](https://miro.medium.com/max/600/1*gn5q2deBej2vLLuFzMwLNA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Task: Implementing Wasserstein distance for a discrete distribution\n",
    "\n",
    "The general formula for Wasserstain distance is:\n",
    "\n",
    "$$W_p(p, q) = \\inf\\limits_{\\pi \\in \\Gamma(u, v)} \\int\\limits_{\\mathbb{M} \\times \\mathbb{M}} d(x, y)^p d \\pi(x, y)$$\n",
    "\n",
    "\n",
    "We are going to discuss the most straightforward case with $p=1$, i.e. pure Earth Moving Distance.\n",
    "\n",
    "In this case, the formula is simplified:\n",
    "\n",
    "$$W_1(p, q) = \\inf\\limits_{\\pi \\in \\Gamma(u, v)}  \\int\\limits_{\\mathbb{R} \\times \\mathbb{R}} |x - y| \\pi(x, y)$$\n",
    "\n",
    "The physical meaning: how to make a minimal amount of work to transfer earth from one heap to another:\n",
    "![](https://sbl.inria.fr/fig/Earth_mover_distance_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that `scipy.stats.wasserstein_distance` takes as an input distribution and not a probability density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d26e1f457c6a709fb6f6f35eb543b0b",
     "grade": false,
     "grade_id": "cell-5572aeb3cfb43d82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def wasserstein_distance_for_pdf(p_probs, q_probs):\n",
    "    \"\"\"\n",
    "    Task is to implement wasserstein distance computation for pdfs with scipy.stats.wasserstein_distance\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "750b22598a6f094fb28775b371a2dab9",
     "grade": true,
     "grade_id": "wasserstein_distance_for_pdf",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(\n",
    "    wasserstein_distance_for_pdf(torch.full((10, ), 1.0 / 10), torch.full((10, ), 1.0 / 10)).item(),\n",
    "    0.,\n",
    ")\n",
    "\n",
    "np_testing.assert_almost_equal(\n",
    "    wasserstein_distance_for_pdf(torch.full((10, ), 1.0 / 10), torch.arange(10).float() / torch.arange(10).sum()).item(),\n",
    "    0.18333333563059567,\n",
    ")\n",
    "\n",
    "np_testing.assert_almost_equal(\n",
    "    wasserstein_distance_for_pdf(torch.arange(9, -1, -1).float() / 45, torch.arange(10).float() / 45).item(),\n",
    "    0.3666666692122817,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not differentiable :(\n",
    "\n",
    "For $W_1$ in 1dim case we have a closed form expression for Wasserstein distance:\n",
    "\n",
    "\n",
    "$$W_1(p, q) =  \\int\\limits_{-\\infty}^{+\\infty} |P(x) - Q(x)| dx = \\int\\limits_{-\\infty}^{+\\infty}\\left|\\int\\limits_{-\\infty}^{x} P(y) - Q(y)dy \\right| dx$$\n",
    "\n",
    "\n",
    "Very difficult to prove, just belive me, please: \n",
    "\n",
    "https://www.stat.cmu.edu/~larry/=sml/Opt.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f502cf837ee05041e6aa28ba0892a9ba",
     "grade": false,
     "grade_id": "cell-f0aeef5008a5cca2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_wasserstein_distance(p_probs, q_probs):\n",
    "    \"\"\"\n",
    "    Task is to implement wasserstein distance using the formula from above\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e505e865a6182c53155a7d29d7e7c382",
     "grade": true,
     "grade_id": "compute_wasserstein_distance",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(\n",
    "    compute_wasserstein_distance(torch.full((10, ), 1.0 / 10), torch.full((10, ), 1.0 / 10)).item(),\n",
    "    0.,\n",
    ")\n",
    "\n",
    "np_testing.assert_almost_equal(\n",
    "    compute_wasserstein_distance(torch.full((10, ), 1.0 / 10), torch.arange(10).float() / torch.arange(10).sum()).item(),\n",
    "    0.18333333563059567,\n",
    ")\n",
    "\n",
    "np_testing.assert_almost_equal(\n",
    "    compute_wasserstein_distance(torch.arange(9, -1, -1).float() / 45, torch.arange(10).float() / 45).item(),\n",
    "    0.3666666692122817,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that now we have `grad_fn=<ExpBackward>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "binomial_dist = distrs.Binomial(total_count=n, probs=p)\n",
    "binom_data = binomial_dist.log_prob(index).exp()\n",
    "binom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = compute_wasserstein_distance(true_data, binom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because W have  `grad_fn=<MeanBackward0>` we can apply `.backward()` method to compute the gradients w.r.t. `p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Task: Implementing optimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b57041a85c8178b565c952a2ac57e42",
     "grade": false,
     "grade_id": "cell-efe80e17c1a53a67",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "EPS = 1e-3\n",
    "\n",
    "def optimize_binomial_distribution(func, p, true_data, lr: float=1e-3, epochs: int=1000):\n",
    "    \"\"\"\n",
    "    func: distance function that takes two arguments\n",
    "    p: initial guess on parameter p of binomial distribution\n",
    "    true_data: true_data\n",
    "    lr: learning rate\n",
    "    epochs: number of training iterations\n",
    "    \"\"\"\n",
    "    p = p.clone().detach().requires_grad_(True)\n",
    "    history = defaultdict(list)\n",
    "    opt = optim.Adam([p], lr=lr)\n",
    "    \n",
    "    n = len(true_data)\n",
    "    index = torch.arange(n).float()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        p.data.clamp_(EPS, 1 - EPS)\n",
    "        # Here your task is to \n",
    "        # 1. create binomial distribution with Pyro\n",
    "        # 2. calculate p.d.f. of the binomial distribution\n",
    "        # 3. apply func to true_data and p.d.f. from previous step\n",
    "        # 4. perform usual backprop\n",
    "        # Do not forget to zero grad after weights update!\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        history['epoch'].append(i)\n",
    "        history['dist'].append(d.item())\n",
    "        history['p'].append(p.item())\n",
    "\n",
    "    return p, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Task: Fitting with Wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_p, hist = optimize_binomial_distribution(compute_wasserstein_distance, p, true_data)\n",
    "print(optimal_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31484aeb3881ae33f09b60b050cfd871",
     "grade": true,
     "grade_id": "w_div",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np_testing.assert_almost_equal(optimal_p.item(), 0.4665, decimal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_data = distrs.Binomial(total_count=n, probs=optimal_p).log_prob(index).exp().detach()\n",
    "plt.figure()\n",
    "width = 0.3\n",
    "plt.bar(index, true_data, width=width, label='True')\n",
    "plt.bar(index + width, binom_data, width=width, label='Binomial fitted with KL')\n",
    "plt.xlabel('Teeth Number')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(np.arange(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Wasserstein distance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = torch.linspace(0. + EPS, 1. - EPS, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = [\n",
    "    compute_wasserstein_distance(distrs.Binomial(total_count=n, probs=p).log_prob(index).exp(), true_data) for p in ps\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ps, ws, label='Wasserstein distance')\n",
    "plt.title('Probability Distribution of Space Worm Teeth')\n",
    "plt.ylabel('Teeth Number')\n",
    "plt.xlabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting 2D-normal distribution\n",
    "\n",
    "Credits: my collegues that https://github.com/HSE-LAMBDA/DeepGenerativeModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplementary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import MultivariateNormal\n",
    "def plot_2d_dots(dots, color='blue', label='None'):\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.scatter(dots[:, 0], dots[:, 1], s=1, c=color, label=label)\n",
    "\n",
    "\n",
    "def get_parameters(mu=0., sigma=1.):\n",
    "    train_mu = torch.Tensor([mu, mu]).requires_grad_(True)\n",
    "    train_sigma = torch.Tensor([[sigma, 0.0],\n",
    "                                [0.0, sigma]]).requires_grad_(True)\n",
    "    return train_mu, train_sigma\n",
    "\n",
    "def create_distr(mu, sigma):\n",
    "    return distrs.MultivariateNormal(mu, sigma)\n",
    "\n",
    "\n",
    "def sample(d, num):\n",
    "    return d.sample(torch.Size([num]))\n",
    "\n",
    "class MixtureDistribution:\n",
    "    def __init__(self, p1, p2, w=0.5):\n",
    "        self._p1 = p1\n",
    "        self._p2 = p2\n",
    "        self._w = w\n",
    "        \n",
    "    def sample(self, n):\n",
    "        return torch.cat([sample(self._p1, int(n * self._w)), sample(self._p2, n - int(n * self._w))])\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        return (self._w * self._p1.log_prob(x).exp() + (1. - self._w) * self._p2.log_prob(x).exp()).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target distribution\n",
    "\n",
    "Target distribution is a mixture of two 2-dimensionals normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = create_distr(\n",
    "    mu=torch.Tensor([-5, -5]), \n",
    "    sigma=torch.Tensor([[1., 0.0], \n",
    "                        [0.0, 1.]])\n",
    ")\n",
    "P2 = create_distr(\n",
    "    mu=torch.Tensor([4, 3]), \n",
    "    sigma=torch.Tensor([[1., 0.0], \n",
    "                        [0.0, 1.]])\n",
    ")\n",
    "\n",
    "P = MixtureDistribution(P1, P2, 0.5)\n",
    "\n",
    "samples_x = P.sample(2000)\n",
    "px = P.log_prob(samples_x).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_2d_dots(samples_x, color=px, label='Target distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters()\n",
    "\n",
    "Q = create_distr(train_mu, train_sigma)\n",
    "samples_q = sample(Q, 1000)\n",
    "plt.figure()\n",
    "plot_2d_dots(samples_x, color='r', label='Target distribution: P(x)')\n",
    "plot_2d_dots(samples_q, color= Q.log_prob(samples_q).exp().detach(), label='Search distribution: Q(x)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\PP{\\mathbb{P}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Task: optimization of Wasserstein distance in 2D\n",
    "\n",
    "\n",
    "The equation for the Wasserstein distance is highly intractable. \n",
    "\n",
    "\\begin{equation*}\n",
    "    W_p(p_x , q_y)=\\inf_{{\\gamma \\in \\Pi (x, y)}}\\int D(x,y)^p \\gamma (x,y) dx dy,\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "Using the Kantorovich-Rubinstein duality, we can simplify the calculation to:\n",
    "\n",
    "$\\newcommand\\EE{\\mathbb{E}}$\n",
    "\\begin{equation}\n",
    "W(p, q) = \\sup_{\\|f\\|_L \\leq 1} \\EE_{x \\sim p}[f(x)] - \\EE_{x \\sim q}[f(x)]\n",
    "\\end{equation}\n",
    "\n",
    "where sup is the least upper bound and f is a 1-Lipschitz function, i.e. for some $K$:\n",
    "\n",
    "$$||f(x) - f(y)|| \\leq K ||x - y||$$\n",
    "\n",
    "So to calculate the Wasserstein distance, we just need to find a 1-Lipschitz function. Like other deep learning problem, we can build a deep network to learn it. \n",
    "\n",
    "![WGAN](https://guimperarnau.com/files/blog/Fantastic-GANs-and-where-to-find-them-II/roll_safe_GANs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Firstly, we are going to generate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_p = n_q = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_q = sample(Q, n_q)\n",
    "samples_p = P.sample(n_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([samples_p, samples_q]).view(-1, 2)\n",
    "y = torch.cat([torch.ones(len(samples_p)), \n",
    "               torch.zeros(len(samples_q))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=.2, random_state=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How to ensure Lipschitz conditions on network? \n",
    "### 2.2 Weight clipping\n",
    "\n",
    "$$\\mathrm{network\\_weights} = \\mathrm(\\mathrm{network\\_weights}, c, -c)$$\n",
    "\n",
    "__Quote from the research paper:__ Weight clipping is a clearly terrible way to enforce a Lipschitz constraint. If the clipping parameter is large, then it can take a long time for any weights to reach their limit, thereby making it harder to train the critic till optimality. If the clipping is small, this can easily lead to vanishing gradients when the number of layers is big, or batch normalization is not used (such as in RNNs) â€¦ and we stuck with weight clipping due to its simplicity and already good performance.\n",
    "\n",
    "https://arxiv.org/pdf/1701.07875.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3492ba126216d7ed97f559bd4ecca917",
     "grade": false,
     "grade_id": "cell-c951d25d8a0fff50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def clamp_weights(net, clamp=1e-1):\n",
    "    \"\"\"\n",
    "    This function takes a network and clip all weights of the network. \n",
    "    I.e. -clamp < weight_i < clamp\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4588f02eae9f130bcac2d6b231a928b",
     "grade": true,
     "grade_id": "clamp_weights",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "critic = nn.Sequential(\n",
    "        nn.Linear(2, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 1),\n",
    ")\n",
    "clamp_weights(critic, clamp=1e-4)\n",
    "for param in critic.parameters():\n",
    "    assert (param.data.abs() < 2e-4).all().item() == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said erlier to find function f we need to find sup. We can approximate sup with gradient descent by maximizing following expression:\n",
    "\n",
    "$\\newcommand\\EE{\\mathbb{E}}$\n",
    "\\begin{equation}\n",
    " \\EE_{x \\sim p}[f(x)] - \\EE_{x \\sim q}[f(x)]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "![](https://matthewmcateer.me/static/f25ae3c02f9f3b3ae46e71f6e9aa0013/39600/gan_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c87b8e2878ca626aabfbbb63e2f063d9",
     "grade": false,
     "grade_id": "cell-078e271af0e0d3b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_critic(x, y, input_dim=2, epochs=5):\n",
    "    \"\"\"\n",
    "    This function create and train critic network with weights clipping\n",
    "    \"\"\"\n",
    "    critic = nn.Sequential(\n",
    "        nn.Linear(input_dim, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 1),\n",
    "    )\n",
    "    critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "    for _ in range(epochs):\n",
    "        critic_optim.zero_grad()\n",
    "        preds = critic(x)\n",
    "        # Here your task is to \n",
    "        # implement loss function \\EE_{x \\sim p}[f(x)] - \\EE_{x \\sim q}[f(x)]\n",
    "        # it could be one-line solution btw\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        loss.backward()\n",
    "        critic_optim.step()\n",
    "        clamp_weights(critic, 1e-1)\n",
    "    return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(d, num):\n",
    "    \"\"\"\n",
    "    Sample from distribution num samples with __Pyro__\n",
    "    \"\"\"\n",
    "    res = pyro.sample(\"dist\", d.expand([num]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da15eb6b72fe8e249212c25efcca55ed",
     "grade": false,
     "grade_id": "cell-3a1302394da92bdf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0., sigma=1.)\n",
    "optim = torch.optim.SGD([train_mu, train_sigma], lr=1e-1)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    optim.zero_grad()\n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    samples_q = sample(Q, n_q).detach()\n",
    "    x = torch.cat([samples_p, samples_q])\n",
    "    y = torch.cat([torch.ones(len(samples_p)), \n",
    "                   torch.zeros(len(samples_q))])\n",
    "    \n",
    "    critic = create_critic(x, y)\n",
    "    \n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    samples_q = sample(Q, n_q)\n",
    "    \n",
    "    # Here your task is to \n",
    "    # minimize wasserstein distance \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 1 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'KL={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(samples_p.detach(), color='r', label='Target distribution: P(x)')\n",
    "        q_sample = sample(Q, 1000)\n",
    "        plot_2d_dots(q_sample.detach(), color= Q.log_prob(q_sample).exp().detach(), label='Search distribution: Q(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: \n",
    "\n",
    "You should observe oscillation of the search distribution. It is like trying to move in one node, then it changes its mind try to go into another mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### _Not working, huh?_\n",
    "\n",
    "The problem is in weights clipping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weight_of_network = torch.cat([param.view(-1) for param in critic.parameters()]).detach()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(all_weight_of_network)\n",
    "plt.title(\"Distribution of weights in the network\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient penalty\n",
    "\n",
    "_Genious idea!_ If function f is 1-Lipschitz function then gradient $\\nabla_x f(x) = 1$ almost everywhere. \n",
    "\n",
    "(Theorem about from original paper)\n",
    "\n",
    "![](https://raw.githubusercontent.com/SchattenGenie/pic-storage/master/wgan_theorem.png)\n",
    "\n",
    "Then our loss function is going to be:\n",
    "\n",
    "![](https://raw.githubusercontent.com/SchattenGenie/pic-storage/master/wgan_loss.png)\n",
    "\n",
    "We implicitly define $P_{\\hat{x}}$ sampling uniformly along straight lines between pairs of points sampled from the data distribution $P_r$ and the generator distribution $P_g$. This is motivated by the fact that the optimal critic contains straight lines with gradient norm 1 connecting coupled points from $P_r$ and $P_g$.\n",
    "\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1704.00028.pdf\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/996/1*2VKp7YH767UPbts1QfHeeA.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af0573559e085b2d5b18b6ff299b1b78",
     "grade": false,
     "grade_id": "cell-801651ed7e45d047",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(critic, x_r, x_q):\n",
    "    \"\"\"\n",
    "    critic: network\n",
    "    x_r, x_q: data sampled form P and Q distributions\n",
    "    \n",
    "    1. ensure that size of x_r and x_q is the same\n",
    "    2. sample random weights from uniform distribution w = [0, 1]\n",
    "    3. x_mixed = w x_r + (1 - w) x_r\n",
    "    4. apply critic to x_mixed\n",
    "    5. calculate gradient of critic output w.r.t. to x_mixed\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return grad_wrt_mixed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b199cd7a48954d92ef9730b0ac0aaff3",
     "grade": true,
     "grade_id": "compute_gradient_penalty",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummpy_critic = nn.Sequential(\n",
    "    nn.Linear(2, 10),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(10, 1)\n",
    ")\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.fill_(0.1)\n",
    "        m.bias.data.fill_(0.1)\n",
    "dummpy_critic.apply(init_weights)\n",
    "np.random.seed(1337)\n",
    "res = compute_gradient_penalty(\n",
    "        dummpy_critic,\n",
    "        torch.tensor([1., 2., 3., 4.]).view(-1, 2),\n",
    "        torch.tensor([1., 2., 3., 4.]).view(-1, 2)\n",
    "    ) - torch.tensor([[0.0856, 0.0856], \n",
    "                      [0.0559, 0.0559]])\n",
    "np_testing.assert_almost_equal(\n",
    "    res.sum(),\n",
    "    0.,\n",
    "    decimal=1e-3\n",
    ")\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fcae12122b1c074d4fc929456966311",
     "grade": false,
     "grade_id": "cell-eeef26c10ee0c1cb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_critic_with_penalty(x, y, input_dim=2, epochs=5):\n",
    "    \"\"\"\n",
    "    This function create and train critic network with weights clipping\n",
    "    \"\"\"\n",
    "    critic = nn.Sequential(\n",
    "        nn.Linear(input_dim, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 1),\n",
    "    )\n",
    "    critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "    for _ in range(epochs):\n",
    "        critic_optim.zero_grad()\n",
    "        preds = critic(x)\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        loss.backward()\n",
    "        critic_optim.step()\n",
    "    return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44841824561f66df93ae135f383e1cbf",
     "grade": false,
     "grade_id": "cell-cd00a7ab172d7339",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_mu, train_sigma = get_parameters(mu=0., sigma=1.)\n",
    "optim = torch.optim.SGD([train_mu, train_sigma], lr=1e-1)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    optim.zero_grad()\n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    samples_q = sample(Q, n_q).detach()\n",
    "    x = torch.cat([samples_p, samples_q])\n",
    "    y = torch.cat([torch.ones(len(samples_p)), \n",
    "                   torch.zeros(len(samples_q))])\n",
    "    \n",
    "    critic = create_critic_with_penalty(x, y)\n",
    "    \n",
    "    Q = create_distr(train_mu, train_sigma)\n",
    "    samples_q = sample(Q, n_q)\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 1 == 0:\n",
    "        # plot pdfs\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.title(f'W={loss.item()}, iter={i}')\n",
    "        plot_2d_dots(samples_p.detach(), color='r', label='Target distribution: P(x)')\n",
    "        q_sample = sample(Q, 1000)\n",
    "        plot_2d_dots(q_sample.detach(), color= Q.log_prob(q_sample).exp().detach(), label='Search distribution: Q(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: \n",
    "\n",
    "You should observe oscillation of the search distribution. At the end search distribution would try to spread and cover both modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weight_of_network = torch.cat([param.view(-1) for param in critic.parameters()]).detach()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(all_weight_of_network)\n",
    "plt.title(\"Distribution of weights in the network\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
