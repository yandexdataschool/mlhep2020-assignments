{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "uxTtyO6DNdgt"
   },
   "source": [
    "# Plan\n",
    "## This notebook\n",
    "1.   Walk through the boilerplate code\n",
    "2.   You write your first 1D GAN\n",
    "\n",
    "## Teaser for the next seminar\n",
    "1. Adapt the 1D GAN for 5D conditional BaBar DIRC generation\n",
    "2. Make it into a Wassershtein GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "rdkNX1y7xdOt"
   },
   "source": [
    "__@this notebook__ will guide you through a very simple case of generative adversarial networks. Like.. veeery simple. Generative adversarial network learn to sample distributions. And here we will solve the easiest imaginable task - learning to sample a 1D normal data distribution.\n",
    "\n",
    "This notebook features a lot of useful visualizations that will help you both get acquainted with the behavior of two networks and debug common errors without having to wait hours of GPU time.\n",
    "\n",
    "Inherited from https://github.com/yandexdataschool/mlhep2019/blob/master/notebooks/day-6/06_1D_GAN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "rU--EKi1N_QZ"
   },
   "source": [
    "# Task 1 (difficulty: the easiest GAN we managed to invent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8jryKf-06gb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, norm, kstest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6xVCI7FxdOw"
   },
   "outputs": [],
   "source": [
    "LATENT_DIM = 4\n",
    "MEAN = 5\n",
    "STD = 1.5\n",
    "DATA_DISTRIBUTION = norm(loc=MEAN, scale=STD)\n",
    "def sample_noise(batch_size):\n",
    "    \"\"\"Returns uniform noise of shape [batch_size, LATENT_DIM] in range [0, 1]\"\"\"\n",
    "    return torch.rand(batch_size, LATENT_DIM, device=\"cuda\")\n",
    "\n",
    "def sample_real_data(batch_size):\n",
    "    \"\"\"\n",
    "    Returns a sample of the target distribution -\n",
    "    Gaussian(mu=MEAN, std=1.5) of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    return torch.randn(batch_size, 1, device=\"cuda\") * STD + MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vda8pfdGxdO1"
   },
   "outputs": [],
   "source": [
    "# Generator converts noise into 1D data\n",
    "gen = nn.Sequential(nn.Linear(LATENT_DIM, 16), nn.ELU(),\n",
    "                    nn.Linear(16, 16), nn.ELU(),\n",
    "                    nn.Linear(16, 1)).cuda()\n",
    "gen_opt = torch.optim.SGD(gen.parameters(), lr=1e-2)\n",
    "\n",
    "# Discriminator converts data into a single number, whose\n",
    "# softmax is the probability of the example being real.\n",
    "# It is deliberately made stronger than generator to make sure disc\n",
    "# is slightly \"ahead in the game\".\n",
    "disc = nn.Sequential(nn.Linear(1, 32), nn.ELU(),\n",
    "                     nn.Linear(32, 32), nn.ELU(),\n",
    "                     nn.Linear(32, 1)).cuda()\n",
    "disc_opt = torch.optim.SGD(disc.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "equjT80PmWYY"
   },
   "source": [
    "OK, now that we have our model defined, we need our loss functions. Historically the first loss used in GANs is the cross-entropy that we already used so many times:\n",
    "$$\\mathscr{L}^{\\text{discr}} =\n",
    "-\\mathbb{E}\\log\\left[P(\\text{real is real})\\right]\n",
    "-\\mathbb{E}\\log\\left[P(\\text{fake is fake})\\right]\n",
    "=\n",
    "-\\mathbb{E}\\left[\\log D(x_\\text{real})\\right]\n",
    "-\\mathbb{E}\\left[\\log (1 - D(x_\\text{fake}))\\right] \n",
    "$$\n",
    "\n",
    "The generator loss presents a choice. We can either maximize the likelihood that generated examples are labeled as real, or minimize the likelihood that they are labeled false.\n",
    "\n",
    "$$\\mathscr{L}^{\\text{gen}} =\n",
    "-\\mathbb{E}\\log\\left[P(\\text{fake is real})\\right] =\n",
    "-\\mathbb{E}\\left[\\log (D(x_\\text{fake}))\\right]$$\n",
    "\n",
    "OR\n",
    "\n",
    "$$\\mathscr{L}^{\\text{gen}} =\n",
    "\\mathbb{E}\\log\\left[P(\\text{fake is fake})\\right] =\n",
    "\\mathbb{E}\\left[\\log (1 - D(x_\\text{fake}))\\right]$$\n",
    "\n",
    "**Question to you.** Which way do think is better?\n",
    "\n",
    "Note that here $D(x)$ is the probability the discriminator assigns to $x$ to be from the real dataset, don't forget to apply sigmoid to the NN's output: $D(x) = \\sigma(\\text{discriminator}(x))$. Try implementing these loss functions below. You should use the `logsigmoid` as a stable realization of $log\\left[\\sigma(x)\\right]$. Note that $1 - \\sigma(x) = \\sigma(-x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "-jEB_ViCxdO6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02ae7d52e70e42a6b7192aea3a8ba3a8",
     "grade": false,
     "grade_id": "f083cb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_disc(batch_size):\n",
    "    \"\"\"\n",
    "    Trains the discriminator for one step. Please note\n",
    "    this is not a pure function, it captutes the majority of variables\n",
    "    from the context.\n",
    "    \"\"\"\n",
    "\n",
    "    # compute log[p(real | x)]\n",
    "    real_data = sample_real_data(batch_size)\n",
    "    logp_real_is_real = F.logsigmoid(disc(real_data))\n",
    "\n",
    "    # there is a function in the begining of the notebook...\n",
    "    # noise = <sample noise>\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # supply the generated noise sample as input to the\n",
    "    # generator network\n",
    "    # gen_data = <generate data given noise>\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # logp_gen_is_fake = <compute log[p(example is fake)]>\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    # Note that sigmoid from the discrimiantor output is\n",
    "    # the probability that the example is real. The definition of sigmoid\n",
    "    # https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "    # and the text above this cell\n",
    "    # are your friends here for an elegant expression.\n",
    "\n",
    "    # You want the discriminator to maximize the probabilities\n",
    "    # of real examples being labeled real and fake examples being labeled fake\n",
    "    # L_D = - log[p(real is real)] - log[p(generated is fake.mean()\n",
    "    # disc_loss = <disc_loss>\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # SGD step. We zero_grad first to clear any gradients left from generator training\n",
    "    disc_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "jWBI4zL-xdO-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecfc7909e9de3c0e981dc025de55305c",
     "grade": false,
     "grade_id": "0e8bfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_gen(batch_size):\n",
    "    \"\"\"Trains generator for one step\"\"\"\n",
    "\n",
    "    # sample the generator\n",
    "    # noise = <sample noise>\n",
    "    # gen_data = <generate data given noise>\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # compute log[p(generated data is real)]\n",
    "    # logp_gen_is_real = <compute log(p(generated example is REAL))>\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # The generator training wants to maximize the probability that\n",
    "    # the generated examples are labeled real by the descriminator\n",
    "    # gen_loss = <generator loss>\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOk5MhVRxdPD"
   },
   "outputs": [],
   "source": [
    "DISCRIMINATOR_ITERATIONS_PER_GENEREATOR = 5\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VALIDATION_BATCH_SIZE = 2048\n",
    "VALIDATION_INTERVAL = 256\n",
    "HIST_BINS=30\n",
    "DATA_HIST_RANGE=[0, 10]\n",
    "\n",
    "data_linspace_np = np.linspace(0, 10, num=256, dtype=np.float32)\n",
    "data_linspace_torch = torch.from_numpy(data_linspace_np)[:, None].cuda()\n",
    "\n",
    "for i in range(10000):\n",
    "    for _ in range(DISCRIMINATOR_ITERATIONS_PER_GENEREATOR):\n",
    "        train_disc(TRAIN_BATCH_SIZE)\n",
    "\n",
    "    train_gen(TRAIN_BATCH_SIZE)\n",
    "\n",
    "    if i % VALIDATION_INTERVAL == 0:\n",
    "        clear_output(True)\n",
    "        validation_noise = sample_noise(VALIDATION_BATCH_SIZE)\n",
    "        validation_data = sample_real_data(VALIDATION_BATCH_SIZE)\n",
    "        validation_generated = gen(validation_noise)\n",
    "\n",
    "        validation_generated_np = validation_generated.data.cpu().numpy()\n",
    "        validation_data_np = validation_data.data.cpu().numpy()\n",
    "\n",
    "        fig, (ax_data, ax_losses) = plt.subplots(ncols=2, figsize=[14, 6])\n",
    "        ax_data.set_title(\"Data distributions\")\n",
    "        ax_data_plots = []\n",
    "        ax_data_plots.append(ax_data.hist(validation_generated_np, range=DATA_HIST_RANGE,\n",
    "                             alpha=0.5, density=True, label='Generated', bins=HIST_BINS)[2][0])\n",
    "        ax_data_plots.append(ax_data.hist(validation_data_np, range=DATA_HIST_RANGE,\n",
    "                             alpha=0.5, density=True, label='Real', bins=HIST_BINS)[2][0])\n",
    "        ax_data.set_xlabel(\"x\")\n",
    "        ax_data.set_ylabel(\"data & generated distributions\")\n",
    "        disc_preal_np = torch.sigmoid(disc(data_linspace_torch)).data.cpu().numpy()\n",
    "        ax_disc = ax_data.twinx()\n",
    "        ax_data_plots.extend(ax_disc.plot(data_linspace_np, disc_preal_np, label=\"discriminator's P(x is real)\"))\n",
    "        ax_disc.set_ylabel(\"discriminator's P(x is real)\")\n",
    "        ax_data.legend(ax_data_plots, [o.get_label() for o in ax_data_plots], loc=\"best\")\n",
    "\n",
    "        ax_losses.set_title(\"Discriminator readout distribution\")\n",
    "        ax_losses.hist(torch.sigmoid(disc(validation_generated)).data.cpu().numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(G(z)) //  generated data', bins=HIST_BINS, density=True)\n",
    "        ax_losses.hist(torch.sigmoid(disc(validation_data)).data.cpu().numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(x) // real data', bins=HIST_BINS, density=True)\n",
    "        ax_losses.legend()\n",
    "        ax_losses.set_xlabel(\"Discriminator output\")\n",
    "        ks_result = kstest(validation_generated_np.ravel(), DATA_DISTRIBUTION.cdf)\n",
    "        fig.suptitle(f\"Iteration {i}, Kolmogorov-Smirnov statistic {ks_result.statistic:.3}, p-value {ks_result.pvalue:.3}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3754c642ed051a25630bcc63d05b0b8e",
     "grade": true,
     "grade_id": "9d2e86",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note, the test uses a larger VALIDATION_BATCH_SIZE than the monitoring in the loop\n",
    "validation_noise = sample_noise(100*VALIDATION_BATCH_SIZE)\n",
    "validation_generated = gen(validation_noise)\n",
    "validation_generated_np = validation_generated.data.cpu().numpy()\n",
    "ks_result = kstest(validation_generated_np.ravel(), DATA_DISTRIBUTION.cdf)\n",
    "assert(ks_result.statistic) < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "MGZnGUQb0Boj"
   },
   "source": [
    "Question to you. Do you think adding dropout will improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "4mAP_q6txdPK"
   },
   "source": [
    "__What to expect:__\n",
    "* __left:__ two distributions will start differently, but generator distribution should match real data _almost_ everywhere. The curve represents discriminator's opinion on all possible values of x. It should slowly get closer to 0.5 over areas where real data is dense.\n",
    "* __right:__ this chart shows how frequently does discriminator assign given probability to samples from real and generated data samples (shown in different colors). First several iterations will vary, but eventually they will both have nearly all probability mass around 0.5 as generator becomes better at it's job.\n",
    " * If instead it converges to two delta-functions around 0(gen) and 1(real) each, your discriminator has won. _Check generator loss function_. As a final measure, try decreasing discriminator learning rate. This can also happen if you replace mean over batch with sum or similar.\n",
    " * If it converges to 0.5 and stays there for several iterations but generator haven't learned to generate plausible data yet, generator is winning the game. _Double-check discriminator loss function_.\n",
    " \n",
    " __Reference plots for a trained GAN__:\n",
    " ![Reference plots for a trained GAN](https://github.com/yandexdataschool/mlhep2019/raw/master/notebooks/day-6/trained_1D_GAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "8k-YfpeBgj5l"
   },
   "source": [
    "# Task 1.X Bonus (diffuculty: we don't guarantee it's possible)\n",
    "The GAN above transformes 4D latent space into 1D Gaussian.\n",
    "Make a GAN that would transform 1D $\\mathbb U[0, 1]$ space into the same 1D Gaussian.\n",
    "P. S.\n",
    "It is a trivila task for, say, quantile transform. But we speak GAN here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# P. S.\n",
    "If you feel stuck, feel free to take a look at a [completed implementation of JS GAN](https://github.com/kazeevn/mnist-gan-demo/blob/master/GAN%20training.ipynb), that we used to produce the [demo](https://en.pelican.study/classroom/213/dialogs/2619/run/). In the opposite case of finishing the assignment, you might still want to take a look. Most of the GAN applications are related to images - and the MNIST GAN is an image GAN."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "06_1D_GAN_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
