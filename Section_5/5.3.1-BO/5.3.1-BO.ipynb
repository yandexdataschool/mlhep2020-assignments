{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 12345678\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = np.arange(5)\n",
    "\n",
    "model = [\n",
    "    lambda x: np.abs(x - 0),\n",
    "    lambda x: np.abs(x - 1),\n",
    "    lambda x: np.abs(x - 2),\n",
    "    lambda x: np.abs(x - 3),\n",
    "    lambda x: np.abs(x - 4),\n",
    "    \n",
    "    lambda x: np.abs(x - 0) + 1,\n",
    "    lambda x: np.abs(x - 1) + 1,\n",
    "    lambda x: np.abs(x - 2) + 1,\n",
    "    lambda x: np.abs(x - 3) + 1,\n",
    "    lambda x: np.abs(x - 4) + 1,\n",
    "    \n",
    "    lambda x: np.abs(x - 1) + 2,\n",
    "    lambda x: np.abs(x - 2) + 2,\n",
    "    lambda x: np.abs(x - 3) + 2,\n",
    "    \n",
    "    lambda x: np.abs(x - 2) + 3,\n",
    "    \n",
    "    lambda x: (x - 1) ** 2,\n",
    "    lambda x: (x - 2) ** 2,\n",
    "    lambda x: (x - 3) ** 2,\n",
    "    \n",
    "    lambda x: 9 - (x - 1) ** 2,\n",
    "    lambda x: 4 - np.abs(x - 3),\n",
    "    \n",
    "    lambda x: (x - 2) ** 2 + 1,\n",
    "]\n",
    "\n",
    "prior = np.ones(len(model)) / len(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(xs, fs, posterior, X=None, Y=None):\n",
    "    X = X if X is not None else []\n",
    "    Y = Y if Y is not None else []\n",
    "\n",
    "    alphas = posterior / np.max(posterior)\n",
    "    \n",
    "    plt.xticks(support)\n",
    "    plt.yticks(np.arange(10))\n",
    "    plt.grid(which='major')\n",
    "    plt.title('H = %.3lf' % (entropy(posterior), ))\n",
    "    \n",
    "    for i, f in enumerate(fs):\n",
    "        color = plt.cm.tab20(i)\n",
    "        plt.plot(xs, f(xs), color=color, alpha=alphas[i])\n",
    "    \n",
    "    plt.scatter(X, Y, marker='x', color='black')\n",
    "    plt.xlabel('$x$', fontsize=14)\n",
    "    plt.ylabel('$f(x)$', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plot_model(support, model, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation(target, x):\n",
    "    y = target(x)\n",
    "    \n",
    "    if np.random.binomial(1, 0.5) == 0:\n",
    "        return y\n",
    "    else:\n",
    "        delta = 2 * np.random.randint(2) - 1\n",
    "        return y + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = model[-1]\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(support, f(support), '--', color=plt.cm.tab10(0))\n",
    "\n",
    "for dx, p in zip([-1, 0, +1], [0.25, 0.5, 0.25]):\n",
    "    plt.scatter(support, f(support) + dx, color=plt.cm.tab10(0), alpha=p)\n",
    "\n",
    "plt.xlabel('$x$', fontsize=14)\n",
    "plt.ylabel('$f(x)$', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(f, X, Y):\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    deltas = np.abs(f(X) - Y)\n",
    "\n",
    "    if np.any(deltas > 1):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.prod(np.where(deltas == 0, 0.5, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Baysian Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(fs, X, Y, prior):\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "\n",
    "    posterior = np.zeros_like(prior)\n",
    "    \n",
    "    for i, f in enumerate(fs):\n",
    "        posterior[i] = prior[i] * get_likelihood(f, X, Y)\n",
    "    \n",
    "    return posterior / np.sum(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook, we work with look-ahead acquisition functions. They select point that minimizes expected criterion:\n",
    "$$x' = \\mathrm{arg\\,max}_{x} \\big[ \\mathbb{E}_{f, y} \\mathrm{criterion}(X, Y) - \\mathrm{criterion}(X \\cup \\{x\\}, Y \\cup \\{y\\}) \\big]$$\n",
    "\n",
    "*$\\mathrm{criterion}(X, Y)$ was added so the acqusition function can be interpreted as a gain of the criterion. For example, when we use an entropy-based criterion than the corresponding look-ahead acqusition function measures information.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead_acqusition(criterion):\n",
    "    def acq(xs, fs, posterior):\n",
    "        ### the baseline\n",
    "        H_0 = criterion(xs, fs, posterior)\n",
    "        \n",
    "        gains = np.zeros_like(xs, dtype='float')\n",
    "        \n",
    "        ### for each x we compute the expected gain if x is probed.\n",
    "        for i, x in enumerate(xs):\n",
    "            ### modelling all possible outcomes\n",
    "            for j, f in enumerate(fs):\n",
    "                for delta, p_delta in zip([-1, 0, +1], [0.25, 0.5, 0.25]):\n",
    "                    y = f(x) + delta\n",
    "                    \n",
    "                    ### probability of the outcome\n",
    "                    p_observation = posterior[j] * p_delta\n",
    "                    \n",
    "                    ### nearly impossible outomes might\n",
    "                    ### bring numerical instability\n",
    "                    if p_observation < 1e-6:\n",
    "                        continue\n",
    "                    \n",
    "                    new_posterior = get_posterior(fs, [x], [y], prior=posterior)\n",
    "                    H = criterion(xs, fs, new_posterior)\n",
    "                    gains[i] += (H_0 - H) * p_observation\n",
    "\n",
    "        return gains\n",
    "\n",
    "    return acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gains(supp, gains):\n",
    "    plt.bar(supp, gains, color=plt.cm.Set1(2))\n",
    "\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('expected gain', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best(gains):\n",
    "    max_gain = np.max(gains)\n",
    "    indx_best, = np.where(gains >= max_gain)\n",
    "    return np.random.choice(indx_best)\n",
    "\n",
    "class BayesianOptimization(object):\n",
    "    def __init__(self, xs, fs, prior, acqusition_f):\n",
    "        self.xs = xs\n",
    "        self.fs = fs\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.posterior = prior\n",
    "        self.acqusition_f = acqusition_f\n",
    "        \n",
    "        self.X = list()\n",
    "        self.Y = list()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.posterior = self.prior\n",
    "        \n",
    "        self.X = list()\n",
    "        self.Y = list()\n",
    "    \n",
    "    def tell(self, x, y):\n",
    "        self.posterior = get_posterior(self.fs, [x], [y], prior=self.posterior)\n",
    "\n",
    "        self.X.append(x)\n",
    "        self.Y.append(y)\n",
    "    \n",
    "    def ask(self,):\n",
    "        gains = self.acqusition_f(self.xs, self.fs, self.posterior)\n",
    "        best_indx = select_best(gains)\n",
    "        \n",
    "        return gains, self.xs[best_indx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_distribution(xs, fs, posterior):\n",
    "    \"\"\"\n",
    "    Computes distribution of `min f`.\n",
    "    xs - support of the functions `fs`;\n",
    "    fs - all possible functions (model);\n",
    "    posterior - posterior distribution of functions.\n",
    "\n",
    "    Returns support and distribution of minima.\n",
    "    \"\"\"\n",
    "    distribution = dict()\n",
    "\n",
    "    for i, f in enumerate(fs):\n",
    "        min_y = np.min(f(xs))\n",
    "        distribution[min_y] = distribution.get(min_y, 0.0) + posterior[i]\n",
    "    \n",
    "    ys = np.array(list(distribution.keys()))\n",
    "    min_y, max_y = np.min(ys), np.max(ys)\n",
    "    \n",
    "    supp = np.arange(min_y, max_y + 1)\n",
    "    \n",
    "    return supp, np.array([distribution.get(y, 0.0) for y in supp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_min_distribution(supp, distribution):\n",
    "    plt.barh(supp, distribution, color=plt.cm.Set1(3))\n",
    "    \n",
    "    plt.title('H = %.3lf' % (entropy(distribution, )))\n",
    "    plt.xlabel('probability of min', fontsize=14)\n",
    "    plt.ylabel('y', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argmin_distribution(xs, fs, posterior):\n",
    "    \"\"\"\n",
    "    Computes distribution of `argmin f`.\n",
    "    xs - support of the functions `fs`;\n",
    "    fs - all possible functions (model);\n",
    "    posterior - posterior distribution of functions.\n",
    "\n",
    "    Returns support and distribution of locations of minima.\n",
    "    \"\"\"\n",
    "    distribution = np.zeros_like(xs, dtype='float')\n",
    "\n",
    "    for i, f in enumerate(fs):\n",
    "        index_min = np.argmin(f(xs))\n",
    "        distribution[index_min] += posterior[i]\n",
    "    \n",
    "    return xs, distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_argmin_distribution(supp, distribution):\n",
    "    plt.bar(supp, distribution, color=plt.cm.Set1(1))\n",
    "    \n",
    "    plt.title('H = %.3lf' % (entropy(distribution, )))\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('probability of argmin', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_info(xs, fs, posterior, X, Y, gains):\n",
    "    plt.subplots(2, 2, figsize=(9, 9))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plot_model(xs, model, posterior, X, Y)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    supp, distr = get_min_distribution(xs, fs, posterior)\n",
    "    plot_min_distribution(supp, distr)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    supp, distr = get_argmin_distribution(xs, fs, posterior)\n",
    "    plot_argmin_distribution(supp, distr)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plot_gains(xs, gains)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [2, 3]\n",
    "Y_test = [2, 1]\n",
    "\n",
    "posterior_test = get_posterior(model, X_test, Y_test, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_info(support, model, posterior_test, X_test, Y_test, np.ones_like(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_minimize(ground_truth, support, model, prior, acqusition_f, plot=False):\n",
    "    bo = BayesianOptimization(support, model, prior=prior, acqusition_f=acqusition_f)\n",
    "    \n",
    "    gain = 1\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    while gain > 1e-3:\n",
    "        if plot:\n",
    "            print('iteration %d' % (iteration, ))\n",
    "\n",
    "        gains, x = bo.ask()\n",
    "        gain = np.max(gains)\n",
    "        y = get_observation(ground_truth, x)\n",
    "        \n",
    "        if plot:\n",
    "            plot_info(support, model, bo.posterior, bo.X, bo.Y, gains)\n",
    "            plt.show()\n",
    "\n",
    "        bo.tell(x, y)\n",
    "\n",
    "        iteration += 1\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plot_model(support, model, bo.posterior, X=bo.X, Y=bo.Y)\n",
    "        plt.show()\n",
    "    \n",
    "    return bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Entropy Model Search\n",
    "\n",
    "*(It is not a conventional name.)*\n",
    "\n",
    "In entropy model search , we aim at minimizing entropy of posterior distribution of the model:\n",
    "\n",
    "$$\\mathrm{criterion} = \\mathbb{H}\\big[ P\\big(F \\mid X, y\\big) \\big]$$\n",
    "\n",
    "Notice, that it is not neccesseraly the most efficient criterion, since it aims to recover the whole objective function, not only position of the minimum. In other words, this acquisition function has no concerns about optimization. *It is purely explorative.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_model(xs, fs, posterior):\n",
    "    return entropy(posterior)\n",
    "\n",
    "model_search = lookahead_acqusition(entropy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selecting the ground-truth\n",
    "model_rng = np.random.RandomState(seed=12345)\n",
    "model_index = model_rng.randint(len(model), )\n",
    "ground_truth = model[model_index]\n",
    "\n",
    "results = entropy_minimize(\n",
    "    ground_truth,\n",
    "    support, model, prior,\n",
    "    acqusition_f=model_search,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rng = np.random.RandomState(seed=12345)\n",
    "\n",
    "for _ in range(10):\n",
    "    ground_truth_model_index = model_rng.randint(len(model), )\n",
    "    ground_truth = model[ground_truth_model_index]\n",
    "\n",
    "    results = entropy_minimize(\n",
    "        ground_truth,\n",
    "        support, model, prior,\n",
    "        acqusition_f=model_search,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    inferred = np.argmax(results.posterior)\n",
    "    assert inferred == ground_truth_model_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Entropy Search\n",
    "\n",
    "*(This time, it is a conventional name.)*\n",
    "\n",
    "Entropy search is only concerned with the location of the minimum:\n",
    "\n",
    "$$\\mathrm{criterion} = \\mathbb{H}\\big[ P\\big(\\mathrm{arg\\,min}_{x} F(x) \\mid X, y\\big) \\big]$$\n",
    "\n",
    "\n",
    "The lookahead acquisition function produced by this criterion is one of the most efficient functions if the goal is to find the position of the minimum without any regards to the rest of objective. Notice, that this function is not concerned with values of the objective function, and does not aim to extact any information about points that are close to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49e9fcb7c82aece732024d905b0e8f71",
     "grade": false,
     "grade_id": "BO-entropy-search",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def entropy_argmin(xs, fs, posterior):\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "entropy_search = lookahead_acqusition(entropy_argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selecting the ground-truth\n",
    "model_rng = np.random.RandomState(seed=12345)\n",
    "model_index = model_rng.randint(len(model), )\n",
    "ground_truth = model[model_index]\n",
    "\n",
    "results = entropy_minimize(\n",
    "    ground_truth,\n",
    "    support, model, prior,\n",
    "    acqusition_f=entropy_search,\n",
    "    plot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823d45c46cb4a5a67c1284cb30e0f061",
     "grade": true,
     "grade_id": "BO-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_rng = np.random.RandomState(seed=12345)\n",
    "\n",
    "for _ in range(10):\n",
    "    ground_truth_model_index = model_rng.randint(len(model), )\n",
    "    ground_truth = model[ground_truth_model_index]\n",
    "\n",
    "    results = entropy_minimize(\n",
    "        ground_truth,\n",
    "        support, model, prior,\n",
    "        acqusition_f=entropy_search,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    inferred = np.argmax(results.posterior)\n",
    "    ### all functions have a single miminum\n",
    "    inferred_argmin = np.argmin(model[inferred](support))\n",
    "    actual_argmin = np.argmin(ground_truth(support))\n",
    "    \n",
    "    \n",
    "    assert inferred_argmin == actual_argmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Entropy Value Search\n",
    "\n",
    "*(Also a conventional name.)*\n",
    "\n",
    "Entropy value search locates value of the minimum:\n",
    "\n",
    "$$\\mathrm{criterion} = \\mathbb{H}\\big[ P\\big(\\mathrm{min}_{x} F(x) \\mid X, y\\big) \\big]$$\n",
    "\n",
    "\n",
    "The look-ahead acquisition function produced by this criterion does not guarantee that optimization convergences (i.e., locates $\\mathrm{argmin}_x f$), however, it might be useful for putting a lower bound on a black-box function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.randint(len(model))\n",
    "ground_truth = model[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84dc236d947f70dffb039dbbc7f2d60e",
     "grade": false,
     "grade_id": "BO-value-search",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def entropy_min(xs, fs, posterior):\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "entropy_value_search = lookahead_acqusition(entropy_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selecting the ground-truth\n",
    "model_rng = np.random.RandomState(seed=12345)\n",
    "model_index = model_rng.randint(len(model), )\n",
    "ground_truth = model[model_index]\n",
    "\n",
    "results = entropy_minimize(\n",
    "    ground_truth,\n",
    "    support, model, prior,\n",
    "    acqusition_f=entropy_value_search,\n",
    "    plot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "410453f952d6557550ceaf949961ade3",
     "grade": true,
     "grade_id": "BO-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_rng = np.random.RandomState(seed=12345)\n",
    "\n",
    "for _ in range(10):\n",
    "    ground_truth_model_index = model_rng.randint(len(model), )\n",
    "    ground_truth = model[ground_truth_model_index]\n",
    "\n",
    "    results = entropy_minimize(\n",
    "        ground_truth,\n",
    "        support, model, prior,\n",
    "        acqusition_f=entropy_value_search,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    inferred = np.argmax(results.posterior)\n",
    "    inferred_min = np.min(model[inferred](support))\n",
    "    actual_min = np.min(ground_truth(support))\n",
    "    \n",
    "    \n",
    "    assert np.abs(inferred_min - actual_min) < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Entropy Position-Value Search?\n",
    "\n",
    "*(It is not a conventional name.)*\n",
    "\n",
    "Nothing stops us from combining entropy search and entopy value search, by considering joint distribution of minima and their locations:\n",
    "\n",
    "$$\\mathrm{criterion} = \\mathbb{H}\\big[ P\\big((\\mathrm{arg\\,min}_{x} F(x) \\mathrm{min}_{x} F(x), ) \\mid X, y\\big) \\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23bd49987f3dd22e2c4f9f9f9cf4b25b",
     "grade": false,
     "grade_id": "BO-min-argmin",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_min_argmin_distribution(xs, fs, posterior):\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "def entropy_min_argmin(xs, fs, posterior):\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "entropy_position_value_search = lookahead_acqusition(entropy_min_argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selecting the ground-truth\n",
    "model_rng = np.random.RandomState(seed=12345)\n",
    "model_index = model_rng.randint(len(model), )\n",
    "ground_truth = model[model_index]\n",
    "\n",
    "results = entropy_minimize(\n",
    "    ground_truth,\n",
    "    support, model, prior,\n",
    "    acqusition_f=entropy_position_value_search,\n",
    "    plot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c02b377e88d06222c63c4c14f1bd8b0e",
     "grade": true,
     "grade_id": "BO-4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_rng = np.random.RandomState(seed=12345)\n",
    "\n",
    "for _ in range(10):\n",
    "    ground_truth_model_index = model_rng.randint(len(model), )\n",
    "    ground_truth = model[ground_truth_model_index]\n",
    "\n",
    "    results = entropy_minimize(\n",
    "        ground_truth,\n",
    "        support, model, prior,\n",
    "        acqusition_f=entropy_position_value_search,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    inferred = np.argmax(results.posterior)\n",
    "    \n",
    "    inferred_min = np.min(model[inferred](support))\n",
    "    actual_min = np.min(ground_truth(support))\n",
    "    \n",
    "    inferred_argmin = np.argmin(model[inferred](support))\n",
    "    actual_argmin = np.argmin(ground_truth(support))\n",
    "    \n",
    "    \n",
    "    assert np.abs(inferred_min - actual_min) < 1e-3\n",
    "    assert inferred_argmin == actual_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
