{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Black-Box optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../../../share/SUSY-small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all, y_all = data[:, 1:], data[:, 0]\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X_all, y_all, test_size=0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5)\n",
    "\n",
    "print('Train     :', X_train.shape, y_train.shape)\n",
    "print('Validation:', X_val.shape, y_val.shape)\n",
    "print('Test      :', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(f, history=None, trajectory=False):\n",
    "    xs = np.linspace(-0.5, 2.5, num=250)\n",
    "    X, Y = np.meshgrid(xs, xs)\n",
    "\n",
    "    Z = np.stack([X.reshape(-1), Y.reshape(-1)], axis=1)\n",
    "    F = f(Z[:, 0], Z[:, 1]).reshape(xs.shape[0], xs.shape[0])\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.ylim([-0.5, 2.5])\n",
    "    plt.xlim([-0.5, 2.5])\n",
    "\n",
    "    plt.contour(\n",
    "        xs, xs, F,\n",
    "        levels=np.linspace(0, 4, num=20),\n",
    "        colors=[plt.cm.tab10(0)]\n",
    "    )\n",
    "    plt.scatter([1], [1], marker='*', s=500, color=plt.cm.tab10(3), zorder=5)\n",
    "    \n",
    "    if history is not None:\n",
    "        plt.scatter(history[:, 0], history[:, 1], color=plt.cm.tab10(1), s=100)\n",
    "        if trajectory:\n",
    "            plt.plot(history[:, 0], history[:, 1], color=plt.cm.tab10(1), lw=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The first objective function\n",
    "\n",
    "The first objective function is one of the most popular benchmark functions --- the Resenbrock function.\n",
    "We slightly modify it by applying $\\chi \\to \\log (1 + \\chi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_resenbrock(x1, x2):\n",
    "    return np.log1p(\n",
    "        (1 - x1) ** 2 + 1 * (x2 - x1 ** 2) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log_resenbrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The second objective function\n",
    "\n",
    "One of use-cases for black-box optimization is hyper-parameter tuning.\n",
    "Here, we measure quality of XGBoost as a function of two continous parameters --- learning rate and regularization coefficient.\n",
    "The dataset is a small version of the [SUSY](https://archive.ics.uci.edu/ml/datasets/SUSY) dataset (loaded above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE, due to problems with XGBoost on cocalc, the objective function was replaced\n",
    "### with the one based on sklearn's GradientBoostingClassifier.\n",
    "### Unfortunately, there is not equivalent of reg_lambda in GradientBoostingClassifier.\n",
    "### Thus, the second dimension (log_reg_lambda) is not used at all.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def eval_xgb_quality(log_learning_rate, log_reg_lambda):\n",
    "    ### NOTE: added -2 to log_learning_rate and log_reg_lambda\n",
    "    learning_rate = np.exp(log_learning_rate - 2)\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        #ccp_alpha=reg_lambda,\n",
    "\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "\n",
    "        subsample=1e-1,\n",
    "        random_state=111\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict_proba(X_val)\n",
    "    \n",
    "    return 1 - accuracy_score(y_val, predictions[:, 1] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### baseline quality is for comparison\n",
    "baseline_XGB = eval_xgb_quality(0, 0)\n",
    "print('XGBoost baseline: %.3lf' % (baseline_XGB,) )\n",
    "print('%.3lf' % (eval_xgb_quality(-1, -2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1: Random Search\n",
    "\n",
    "- implement sampling for Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7584deb72f125a30bd9163308cde9322",
     "grade": false,
     "grade_id": "RS",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _, history = random_search(\n",
    "    log_resenbrock, \n",
    "    bounds=[(-0.5, 2.5), (-0.5, 2.5)],\n",
    "    progress=tqdm,\n",
    "    n_evaluations=1000\n",
    ")\n",
    "\n",
    "assert log_resenbrock(x[0], x[1]) < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log_resenbrock, history, trajectory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cc50b5fd509db5b9dab893f93ef32f5",
     "grade": true,
     "grade_id": "RS-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x, error, history = random_search(eval_xgb_quality, bounds=[(-3, 3), (-3, 3)], progress=tqdm)\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2: numerical gradient estimation\n",
    "\n",
    "- implement numerical gradient estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4efaed4b9d862ddb25699a51628b7de4",
     "grade": false,
     "grade_id": "NG",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _, history = numerical_gradient(\n",
    "    log_resenbrock,\n",
    "    x0=np.array([0.5, 0.5]),\n",
    "    progress=tqdm,\n",
    "    \n",
    "    n_evaluations=2500\n",
    ")\n",
    "\n",
    "assert np.all(np.abs(x - 1) < 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log_resenbrock, history, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ef27b1924b13f7722940b30d2b307b6",
     "grade": true,
     "grade_id": "NG-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Function(object):\n",
    "    def __init__(self, f, max_evals=20):\n",
    "        self.max_evals = max_evals\n",
    "        self.n_evals = 0\n",
    "        self.f = f\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.n_evals >= self.max_evals:\n",
    "            raise Exception('Reached maximal number of evaliations')\n",
    "        else:\n",
    "            self.n_evals += 1\n",
    "\n",
    "        return self.f(*args, **kwargs)\n",
    "    \n",
    "x, _, history = numerical_gradient(\n",
    "    Function(eval_xgb_quality, 20),\n",
    "    x0=np.zeros(2),\n",
    "    \n",
    "    ### learning rate changed to 1e-1,\n",
    "    ### since new function has a much larger \"slope\".\n",
    "    learning_rate=1e-1, h=1e-1,\n",
    "    \n",
    "    n_evaluations=20,\n",
    "    progress=tqdm\n",
    ")\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3: Simulated Annealing\n",
    "\n",
    "Implement simulated annealing:\n",
    "- use normally distribited noise with `std=learning_rate` for generating suggestions:\n",
    "  $$\\begin{eqnarray}\n",
    "  x'_{t + 1} &=& x_t + \\varepsilon;\\\\\n",
    "  \\varepsilon &\\sim& \\mathcal{N}(0, \\sigma^2 = \\mathrm{lr}^2)\n",
    "  \\end{eqnarray}$$\n",
    "\n",
    "- accept new point if:\n",
    "  $$x_{t + 1} = \\begin{cases}\n",
    "      x_{t + 1}, &\\text{ if } f(x'_{t + 1}) < f(x_{t});\\\\\n",
    "      x_{t + 1}, &\\text{ with } P = \\exp\\Big( \\frac{\\big(f(x_{t}) - f(x'_{t + 1})\\big)}{T} \\Big);\\\\\n",
    "      x_t  &\\text{ otherwise} \n",
    "  \\end{cases}$$\n",
    "\n",
    "- try different temperature schedules and learning_rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1135b80d78e80531980dd4bd6a28abf4",
     "grade": false,
     "grade_id": "SA",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _, history = simulated_annealing(\n",
    "    log_resenbrock,\n",
    "    x0=np.array([0.5, 0.5]),\n",
    "    \n",
    "    learning_rate=5e-2, T0=1e-1, T_final=1e-3,\n",
    "    \n",
    "    n_evaluations=250,\n",
    "    seed=1111,\n",
    "    progress=tqdm,\n",
    ")\n",
    "\n",
    "assert np.all(np.abs(x - 1) < 2e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log_resenbrock, history, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d2aa75215bc4d1cba040745ed8c0e63",
     "grade": true,
     "grade_id": "SA-XGB",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Function(object):\n",
    "    def __init__(self, f, max_evals=20):\n",
    "        self.max_evals = max_evals\n",
    "        self.n_evals = 0\n",
    "        self.f = f\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.n_evals >= self.max_evals:\n",
    "            raise Exception('Reached maximal number of evaliations')\n",
    "        else:\n",
    "            self.n_evals += 1\n",
    "\n",
    "        return self.f(*args, **kwargs)\n",
    "    \n",
    "x, _, history = simulated_annealing(\n",
    "    Function(eval_xgb_quality, 20),\n",
    "    x0=np.zeros(2),\n",
    "    \n",
    "    learning_rate=1e-1, T0=1e-1, T_final=1e-3,\n",
    "    \n",
    "    n_evaluations=20,\n",
    "    seed=1111,\n",
    "    progress=tqdm\n",
    ")\n",
    "\n",
    "test_quality = eval_xgb_quality(*x)\n",
    "\n",
    "assert test_quality < baseline_XGB, 'Looks like the optimization algorithm did not improved over baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
